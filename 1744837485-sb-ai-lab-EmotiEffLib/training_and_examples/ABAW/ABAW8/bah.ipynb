{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/HDD6TB/datasets/emotions/ABAW/ABAW_8/ABAW-8th-BAH-train-data/'\n",
    "VIDEO_DIR=DATA_DIR+'videos/'\n",
    "FACES_DIR=DATA_DIR+'cropped-aligned-faces/'\n",
    "AUDIO_DIR=DATA_DIR+'audios/'\n",
    "TRANSCRIPTION_DIR=DATA_DIR+'transcription/'\n",
    "TRAIN_LABELS=DATA_DIR+'default-split/train.txt'\n",
    "VAL_LABELS=DATA_DIR+'default-split/val.txt'\n",
    "ANNOTATION_FRAME_LABELS=DATA_DIR+'annotation.yml'\n",
    "\n",
    "TEST_DATA_DIR = DATA_DIR+'ABAW-8th-BAH-test-data/'\n",
    "TEST_VIDEO_DIR=TEST_DATA_DIR+'videos/'\n",
    "TEST_AUDIO_DIR=TEST_DATA_DIR+'audios/'\n",
    "TEST_FACES_DIR=TEST_DATA_DIR+'cropped-aligned-faces/'\n",
    "TEST_TRANSCRIPTION_DIR=TEST_DATA_DIR+'transcription/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pathlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn import svm,metrics,preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix,precision_score, recall_score, accuracy_score, f1_score,roc_auc_score,average_precision_score\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv  \n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(id):\n",
    "    name = \"\"\n",
    "    if id>=0 and id<10:\n",
    "        name = \"0000\" + str(id)\n",
    "    elif id>=10 and id<100:\n",
    "        name = \"000\" + str(id)\n",
    "    elif id>=100 and id<1000:\n",
    "        name = \"00\" + str(id)\n",
    "    elif id>=1000 and id<10000:\n",
    "        name = \"0\" + str(id)\n",
    "    else:\n",
    "        name = str(id)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_filenames=lambda x: int(os.path.splitext(x)[0].split('-')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.0.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "device = 'cuda'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mbf_va_mtl.pt\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    if True:\n",
    "        model_name='mbf_va_mtl.pt'\n",
    "\n",
    "        IMG_SIZE=112\n",
    "\n",
    "        test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((112,112)),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n",
    "        ]\n",
    "        )\n",
    "\n",
    "        import sys\n",
    "        sys.path.append(\"../../../emotiefflib/backbones/\")\n",
    "        import mobilefacenet\n",
    "    else:\n",
    "        if False:\n",
    "            #model_name='affectnet_vggface2_enet2_gmp_smooth.pt'\n",
    "            model_name='enet_b2_8_best.pt'\n",
    "            #model_name='enet_b2_7.pt'\n",
    "            IMG_SIZE=260 #224 #\n",
    "        else:\n",
    "            #model_name='affectnet_vggface2_enet0.pt'\n",
    "            #model_name='affectnet_vggface2_enet0_new.pt'\n",
    "            #model_name='enet_b0_7.pt'\n",
    "            #model_name='enet_b0_8_best_afew.pt'\n",
    "            #model_name='enet_b0_8_best_vgaf.pt'\n",
    "            model_name='enet_b0_8_va_mtl.pt'\n",
    "            #model_name='mobilevit_va_mtl.pt'\n",
    "\n",
    "            IMG_SIZE=224\n",
    "\n",
    "        #IMG_SIZE=112\n",
    "\n",
    "        test_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "        )\n",
    "        np_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(None),\n",
    "                transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    print(model_name)\n",
    "    feature_extractor_model = torch.load('../../../models/affectnet_emotions/'+model_name)\n",
    "\n",
    "else:\n",
    "    IMG_SIZE=112\n",
    "    \n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])  \n",
    "    \n",
    "    import sys\n",
    "    DDAMNFN_PATH='/home/avsavchenko/src/distr/DDAMFN'\n",
    "    sys.path.append(DDAMNFN_PATH)\n",
    "    from networks.DDAM import DDAMNet\n",
    "\n",
    "    if False:\n",
    "        feature_extractor_model = DDAMNet(num_class=8, num_head=2)\n",
    "        model_name='affectnet8_epoch4_acc0.6462'\n",
    "    else:\n",
    "        feature_extractor_model = DDAMNet(num_class=10, num_head=2)\n",
    "        model_name='affectnet8_epoch9_acc0.642_mtl_2'\n",
    "    feature_extractor_model.load_state_dict(torch.load(DDAMNFN_PATH+'/checkpoints/'+model_name+'.pth')['model_state_dict'])\n",
    "    feature_extractor_model.bn=torch.nn.Identity()\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (features): MobileFaceNet(\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=128)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=128)\n",
       "        )\n",
       "      )\n",
       "      (2): DepthWise(\n",
       "        (layers): Sequential(\n",
       "          (0): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=128)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=128)\n",
       "            )\n",
       "          )\n",
       "          (2): LinearBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Residual(\n",
       "        (layers): Sequential(\n",
       "          (0): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DepthWise(\n",
       "        (layers): Sequential(\n",
       "          (0): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=256)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=256)\n",
       "            )\n",
       "          )\n",
       "          (2): LinearBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Residual(\n",
       "        (layers): Sequential(\n",
       "          (0): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DepthWise(\n",
       "        (layers): Sequential(\n",
       "          (0): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=512)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=512)\n",
       "            )\n",
       "          )\n",
       "          (2): LinearBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Residual(\n",
       "        (layers): Sequential(\n",
       "          (0): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_sep): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "      )\n",
       "    )\n",
       "    (features): GDC(\n",
       "      (layers): Sequential(\n",
       "        (0): LinearBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Flatten()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 512) [[-0.15847237 -0.0082755  -0.05878411 ... -0.13720182 -0.00222744\n",
      "  -0.04625763]\n",
      " [ 0.1631905   0.00305371 -0.0400276  ...  0.04419322  0.07971639\n",
      "   0.02389675]\n",
      " [-0.21113051 -0.00105867 -0.01162707 ... -0.18978879 -0.08599198\n",
      "   0.00453433]\n",
      " ...\n",
      " [ 0.08492246 -0.03406343  0.03888768 ...  0.12722547 -0.0013511\n",
      "   0.09250897]\n",
      " [-0.02396181  0.01268015  0.09095795 ... -0.01691812 -0.06352246\n",
      "   0.06685334]\n",
      " [-0.06165804  0.01331687  0.02941979 ... -0.00387448  0.03024477\n",
      "  -0.00236978]]\n",
      "(10,) [-0.13135104  0.4276935  -0.5028847  -0.40331903  0.56353503  0.45924348\n",
      " -0.16013703  0.10144855  0.16608728  0.13354424]\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    classifier_weights=feature_extractor_model.classifier[0].weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.classifier[0].bias.cpu().data.numpy()\n",
    "elif False:\n",
    "    classifier_weights=feature_extractor_model.classifier.weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.classifier.bias.cpu().data.numpy()\n",
    "elif False:\n",
    "    classifier_weights=feature_extractor_model.head.fc.weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.head.fc.bias.cpu().data.numpy()\n",
    "else:\n",
    "    classifier_weights=feature_extractor_model.fc.weight.cpu().data.numpy()\n",
    "    classifier_bias=feature_extractor_model.fc.bias.cpu().data.numpy()\n",
    "print(classifier_weights.shape,classifier_weights)\n",
    "print(classifier_bias.shape,classifier_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (features): MobileFaceNet(\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=128)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=128)\n",
       "        )\n",
       "      )\n",
       "      (2): DepthWise(\n",
       "        (layers): Sequential(\n",
       "          (0): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=128)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=128)\n",
       "            )\n",
       "          )\n",
       "          (2): LinearBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Residual(\n",
       "        (layers): Sequential(\n",
       "          (0): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=128)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DepthWise(\n",
       "        (layers): Sequential(\n",
       "          (0): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=256)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=256)\n",
       "            )\n",
       "          )\n",
       "          (2): LinearBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Residual(\n",
       "        (layers): Sequential(\n",
       "          (0): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DepthWise(\n",
       "        (layers): Sequential(\n",
       "          (0): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=512)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): PReLU(num_parameters=512)\n",
       "            )\n",
       "          )\n",
       "          (2): LinearBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Residual(\n",
       "        (layers): Sequential(\n",
       "          (0): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): DepthWise(\n",
       "            (layers): Sequential(\n",
       "              (0): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (1): ConvBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): PReLU(num_parameters=256)\n",
       "                )\n",
       "              )\n",
       "              (2): LinearBlock(\n",
       "                (layers): Sequential(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_sep): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "      )\n",
       "    )\n",
       "    (features): GDC(\n",
       "      (layers): Sequential(\n",
       "        (0): LinearBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Flatten()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False:\n",
    "    feature_extractor_model.classifier=torch.nn.Identity()\n",
    "elif False:\n",
    "    feature_extractor_model.head.fc=torch.nn.Identity()\n",
    "else:\n",
    "    feature_extractor_model.fc=torch.nn.Identity()\n",
    "feature_extractor_model=feature_extractor_model.to(device)\n",
    "feature_extractor_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probab(features):\n",
    "    x=np.dot(features,np.transpose(classifier_weights))+classifier_bias\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(112, 112), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n",
      "/home/HDD6TB/datasets/emotions/ABAW/ABAW_8/ABAW-8th-BAH-train-data/cropped-aligned-faces/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9550838c66234ab8bd2b624d8f9d92e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_transforms)\n",
    "data_dir=FACES_DIR\n",
    "print(data_dir)\n",
    "\n",
    "videoname2features,videoname2scores,videoname2img_names={},{},{}\n",
    "for subject in tqdm(sorted(os.listdir(data_dir))):\n",
    "    subject_dir=os.path.join(data_dir,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)   \n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            question_dir=os.path.join(visit_dir,question_name)   \n",
    "            \n",
    "            videoname=subject+'/'+visit_name+'/'+question_name\n",
    "            #print(videoname)\n",
    "\n",
    "            X_features,img_names=[],[]\n",
    "            imgs=[]\n",
    "            for img_name in sorted(os.listdir(question_dir), key=compare_filenames):#[::5]:\n",
    "                #print(video_dir,img_name)\n",
    "                img = Image.open(os.path.join(question_dir,img_name))\n",
    "                img_tensor = test_transforms(img)\n",
    "                if img.size:\n",
    "                    img_names.append(videoname+'/'+img_name)\n",
    "                    imgs.append(img_tensor)\n",
    "                    if len(imgs)>=64: #48: #96: #32:        \n",
    "                        #features,_,_ = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                        features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                        features=features.data.cpu().numpy()\n",
    "                        #print(features.shape)\n",
    "\n",
    "                        if len(X_features)==0:\n",
    "                            X_features=features\n",
    "                        else:\n",
    "                            X_features=np.concatenate((X_features,features),axis=0)\n",
    "                        imgs=[]\n",
    "\n",
    "            if len(imgs)>0:        \n",
    "                #features,_,_ = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                features=features.data.cpu().numpy()\n",
    "\n",
    "                if len(X_features)==0:\n",
    "                    X_features=features\n",
    "                else:\n",
    "                    X_features=np.concatenate((X_features,features),axis=0)\n",
    "\n",
    "                imgs=[]\n",
    "            if len(X_features)>0:\n",
    "                X_scores=get_probab(X_features)\n",
    "                #print(X_features.shape,X_scores.shape)\n",
    "                videoname2features[videoname]=X_features\n",
    "                videoname2scores[videoname]=X_scores\n",
    "                videoname2img_names[videoname]=img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(112, 112), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n",
      "/home/HDD6TB/datasets/emotions/ABAW/ABAW_8/ABAW-8th-BAH-train-data/ABAW-8th-BAH-test-data/cropped-aligned-faces/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5706c5dec99d46cd9242ac019eed161b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_transforms)\n",
    "data_dir=TEST_FACES_DIR\n",
    "print(data_dir)\n",
    "\n",
    "videoname2features_test,videoname2scores_test,videoname2img_names_test={},{},{}\n",
    "for subject in tqdm(sorted(os.listdir(data_dir))):\n",
    "    subject_dir=os.path.join(data_dir,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)   \n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            question_dir=os.path.join(visit_dir,question_name)   \n",
    "            \n",
    "            videoname=subject+'/'+visit_name+'/'+question_name\n",
    "            #print(videoname)\n",
    "\n",
    "            X_features,img_names=[],[]\n",
    "            imgs=[]\n",
    "            for img_name in sorted(os.listdir(question_dir), key=compare_filenames):#[::5]:\n",
    "                #print(video_dir,img_name)\n",
    "                img = Image.open(os.path.join(question_dir,img_name))\n",
    "                img_tensor = test_transforms(img)\n",
    "                if img.size:\n",
    "                    img_names.append(videoname+'/'+img_name)\n",
    "                    imgs.append(img_tensor)\n",
    "                    if len(imgs)>=64: #48: #96: #32:        \n",
    "                        #features,_,_ = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                        features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                        features=features.data.cpu().numpy()\n",
    "                        #print(features.shape)\n",
    "\n",
    "                        if len(X_features)==0:\n",
    "                            X_features=features\n",
    "                        else:\n",
    "                            X_features=np.concatenate((X_features,features),axis=0)\n",
    "                        imgs=[]\n",
    "\n",
    "            if len(imgs)>0:        \n",
    "                #features,_,_ = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                features=features.data.cpu().numpy()\n",
    "\n",
    "                if len(X_features)==0:\n",
    "                    X_features=features\n",
    "                else:\n",
    "                    X_features=np.concatenate((X_features,features),axis=0)\n",
    "\n",
    "                imgs=[]\n",
    "            if len(X_features)>0:\n",
    "                X_scores=get_probab(X_features)\n",
    "                #print(X_features.shape,X_scores.shape)\n",
    "                videoname2features_test[videoname]=X_features\n",
    "                videoname2scores_test[videoname]=X_scores\n",
    "                videoname2img_names_test[videoname]=img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1109, 512) (1109, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_features.shape,X_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bah_mbf_va_aligned.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "if False:\n",
    "    #model_name='enet_b2_8_best'\n",
    "    model_name='enet_b0_8_best_vgaf'\n",
    "    #model_name='ddamfnet_8'\n",
    "    has_va=False\n",
    "else:\n",
    "    #model_name='enet_b0_8_va_mtl'\n",
    "    #model_name='mobilevit_va_mtl'\n",
    "    model_name='mbf_va'\n",
    "    #model_name='ddamfnet_mtl'\n",
    "    has_va=True\n",
    "    \n",
    "MODEL2FEATURES='bah_'+model_name+'_aligned.pickle' \n",
    "#MODEL2FEATURES='bah_ddamfnet_mtl_aligned.pickle'\n",
    "#MODEL2FEATURES='bah_mbf_va_aligned.pickle'\n",
    "#MODEL2FEATURES='bah_mobilevit_va_mtl_aligned.pickle'\n",
    "\n",
    "print(MODEL2FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open(MODEL2FEATURES, 'wb') as handle:\n",
    "        pickle.dump([videoname2features,videoname2scores,videoname2img_names], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 (928, 512) (928, 10)\n"
     ]
    }
   ],
   "source": [
    "filename=MODEL2FEATURES\n",
    "with open(filename, 'rb') as handle:\n",
    "    videoname2features,videoname2scores,videoname2img_names=pickle.load(handle)\n",
    "\n",
    "video_name='82734/Visite_1/82734_Question_5_2024-11-20_16-15-41_Video.mp4'\n",
    "print(len(videoname2features),videoname2features[video_name].shape,videoname2scores[video_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bah_mbf_va_aligned.pickle\n",
      "280 280 280\n"
     ]
    }
   ],
   "source": [
    "MODEL2FEATURES_TEST='test_'+MODEL2FEATURES\n",
    "print(MODEL2FEATURES_TEST)\n",
    "if False:\n",
    "    with open(MODEL2FEATURES_TEST, 'wb') as handle:\n",
    "        pickle.dump([videoname2features_test,videoname2scores_test,videoname2img_names_test], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(MODEL2FEATURES_TEST, 'rb') as handle:\n",
    "        videoname2features_test,videoname2scores_test,videoname2img_names_test=pickle.load(handle)\n",
    "print(len(videoname2features_test),len(videoname2scores_test),len(videoname2img_names_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eri_test_ddamfnet_mtl_orig_faces.pickle\n",
      "4586 (39, 512) (39, 10)\n"
     ]
    }
   ],
   "source": [
    "filename='eri_test_'+model_name+'_orig_faces.pickle'\n",
    "print(filename)\n",
    "with open(filename, 'rb') as handle:\n",
    "    videoname2features_test,videoname2scores_test=pickle.load(handle)\n",
    "print(len(videoname2features_test),videoname2features_test['12660'].shape,videoname2scores_test['12660'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [01:04<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import wave, struct\n",
    "from fnmatch import fnmatch\n",
    "for subject in tqdm(sorted(os.listdir(VIDEO_DIR))):\n",
    "    subject_dir=os.path.join(VIDEO_DIR,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)   \n",
    "        audio_dir=AUDIO_DIR+subject+'/'+visit_name+'/'\n",
    "        if not os.path.exists(audio_dir):\n",
    "            os.makedirs(audio_dir)\n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            video_path=os.path.join(visit_dir,question_name)   \n",
    "            waveFile=audio_dir+question_name+'.wav'\n",
    "            command = \"ffmpeg -i '\"+video_path+\"' -ac 1 -ar 16000 -vn '\"+waveFile+\"'\"\n",
    "            #print(command)\n",
    "            os.system(command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7dd22aaac248dcb3a1825a282b8813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wave, struct\n",
    "from fnmatch import fnmatch\n",
    "for subject in tqdm(sorted(os.listdir(TEST_VIDEO_DIR))):\n",
    "    subject_dir=os.path.join(TEST_VIDEO_DIR,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)   \n",
    "        audio_dir=TEST_AUDIO_DIR+subject+'/'+visit_name+'/'\n",
    "        if not os.path.exists(audio_dir):\n",
    "            os.makedirs(audio_dir)\n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            video_path=os.path.join(visit_dir,question_name)   \n",
    "            waveFile=audio_dir+question_name+'.wav'\n",
    "            command = \"ffmpeg -i '\"+video_path+\"' -ac 1 -ar 16000 -vn '\"+waveFile+\"'\"\n",
    "            #print(command)\n",
    "            os.system(command=command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model,AutoProcessor, HubertModel\n",
    "import torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HubertModel(\n",
       "  (feature_extractor): HubertFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): HubertLayerNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (1-4): 4 x HubertLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x HubertLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): HubertFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): HubertEncoderStableLayerNorm(\n",
       "    (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (padding): HubertSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
       "        (attention): HubertAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): HubertFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False:\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "else:\n",
    "    processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "    model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [15:21<00:00, 10.97s/it]\n"
     ]
    }
   ],
   "source": [
    "videoname2audio_features={}\n",
    "for subject in tqdm(sorted(os.listdir(VIDEO_DIR))):\n",
    "    subject_dir=os.path.join(VIDEO_DIR,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)\n",
    "        audio_dir=AUDIO_DIR+subject+'/'+visit_name+'/'\n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            videoname=subject+'/'+visit_name+'/'+question_name\n",
    "            waveFile=audio_dir+question_name+'.wav'\n",
    "            array, fs = torchaudio.load(waveFile)\n",
    "            inp = processor(array.squeeze(), sampling_rate=fs, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inp)\n",
    "            videoname2audio_features[videoname]=outputs.last_hidden_state.numpy()[0]\n",
    "    #print(videoname2audio_features[videoname].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n"
     ]
    }
   ],
   "source": [
    "#MODEL2AUDIOFEATURES='bah_wav2vec2.pickle'\n",
    "MODEL2AUDIOFEATURES='bah_hubert.pickle'\n",
    "\n",
    "if False:\n",
    "    with open(MODEL2AUDIOFEATURES, 'wb') as handle:\n",
    "        pickle.dump(videoname2audio_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(MODEL2AUDIOFEATURES, 'rb') as handle:\n",
    "        videoname2audio_features=pickle.load(handle)\n",
    "print(len(videoname2audio_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 (482, 1024)\n",
      "730 (730, 1024)\n",
      "835 (835, 1024)\n",
      "610 (610, 1024)\n",
      "681 (681, 1024)\n",
      "1115 (1115, 1024)\n",
      "721 (721, 1024)\n",
      "389 (389, 1024)\n",
      "646 (646, 1024)\n",
      "420 (420, 1024)\n",
      "465 (465, 1024)\n",
      "370 (370, 1024)\n",
      "433 (433, 1024)\n",
      "339 (339, 1024)\n",
      "349 (349, 1024)\n",
      "477 (477, 1024)\n",
      "801 (801, 1024)\n",
      "572 (572, 1024)\n",
      "290 (290, 1024)\n",
      "306 (306, 1024)\n",
      "285 (285, 1024)\n",
      "320 (320, 1024)\n",
      "467 (467, 1024)\n",
      "417 (417, 1024)\n",
      "513 (513, 1024)\n",
      "610 (610, 1024)\n",
      "546 (546, 1024)\n",
      "554 (554, 1024)\n",
      "331 (331, 1024)\n",
      "659 (659, 1024)\n",
      "761 (761, 1024)\n",
      "672 (672, 1024)\n",
      "1236 (1236, 1024)\n",
      "858 (858, 1024)\n",
      "1121 (1121, 1024)\n",
      "806 (806, 1024)\n",
      "1408 (1408, 1024)\n",
      "953 (953, 1024)\n",
      "930 (930, 1024)\n",
      "908 (908, 1024)\n",
      "801 (801, 1024)\n",
      "1391 (1391, 1024)\n",
      "748 (748, 1024)\n",
      "493 (493, 1024)\n",
      "561 (561, 1024)\n",
      "484 (484, 1024)\n",
      "384 (384, 1024)\n",
      "399 (399, 1024)\n",
      "930 (930, 1024)\n",
      "504 (504, 1024)\n",
      "914 (914, 1024)\n",
      "922 (922, 1024)\n",
      "327 (327, 1024)\n",
      "524 (524, 1024)\n",
      "925 (925, 1024)\n",
      "247 (247, 1024)\n",
      "147 (147, 1024)\n",
      "283 (283, 1024)\n",
      "213 (213, 1024)\n",
      "215 (215, 1024)\n",
      "307 (307, 1024)\n",
      "635 (635, 1024)\n",
      "815 (815, 1024)\n",
      "582 (582, 1024)\n",
      "522 (522, 1024)\n",
      "587 (587, 1024)\n",
      "683 (683, 1024)\n",
      "323 (323, 1024)\n",
      "279 (279, 1024)\n",
      "377 (377, 1024)\n",
      "380 (380, 1024)\n",
      "506 (506, 1024)\n",
      "87 (87, 1024)\n",
      "251 (251, 1024)\n",
      "331 (331, 1024)\n",
      "656 (656, 1024)\n",
      "616 (616, 1024)\n",
      "647 (647, 1024)\n",
      "437 (437, 1024)\n",
      "431 (431, 1024)\n",
      "457 (457, 1024)\n",
      "453 (453, 1024)\n",
      "197 (197, 1024)\n",
      "315 (315, 1024)\n",
      "450 (450, 1024)\n",
      "122 (122, 1024)\n",
      "222 (222, 1024)\n",
      "262 (262, 1024)\n",
      "141 (141, 1024)\n",
      "1204 (1204, 1024)\n",
      "1015 (1015, 1024)\n",
      "572 (572, 1024)\n",
      "585 (585, 1024)\n",
      "366 (366, 1024)\n",
      "397 (397, 1024)\n",
      "804 (804, 1024)\n",
      "1302 (1302, 1024)\n",
      "950 (950, 1024)\n",
      "1288 (1288, 1024)\n",
      "927 (927, 1024)\n",
      "1509 (1509, 1024)\n",
      "1373 (1373, 1024)\n",
      "657 (657, 1024)\n",
      "1470 (1470, 1024)\n",
      "833 (833, 1024)\n",
      "911 (911, 1024)\n",
      "919 (919, 1024)\n",
      "778 (778, 1024)\n",
      "889 (889, 1024)\n",
      "245 (245, 1024)\n",
      "536 (536, 1024)\n",
      "247 (247, 1024)\n",
      "544 (544, 1024)\n",
      "363 (363, 1024)\n",
      "501 (501, 1024)\n",
      "665 (665, 1024)\n",
      "1433 (1433, 1024)\n",
      "2204 (2204, 1024)\n",
      "1571 (1571, 1024)\n",
      "1995 (1995, 1024)\n",
      "561 (561, 1024)\n",
      "832 (832, 1024)\n",
      "994 (994, 1024)\n",
      "1092 (1092, 1024)\n",
      "1055 (1055, 1024)\n",
      "790 (790, 1024)\n",
      "1376 (1376, 1024)\n",
      "427 (427, 1024)\n",
      "401 (401, 1024)\n",
      "493 (493, 1024)\n",
      "311 (311, 1024)\n",
      "261 (261, 1024)\n",
      "372 (372, 1024)\n",
      "695 (695, 1024)\n",
      "1128 (1128, 1024)\n",
      "909 (909, 1024)\n",
      "939 (939, 1024)\n",
      "973 (973, 1024)\n",
      "974 (974, 1024)\n",
      "944 (944, 1024)\n",
      "836 (836, 1024)\n",
      "862 (862, 1024)\n",
      "768 (768, 1024)\n",
      "446 (446, 1024)\n",
      "641 (641, 1024)\n",
      "218 (218, 1024)\n",
      "394 (394, 1024)\n",
      "717 (717, 1024)\n",
      "645 (645, 1024)\n",
      "723 (723, 1024)\n",
      "876 (876, 1024)\n",
      "721 (721, 1024)\n",
      "845 (845, 1024)\n",
      "844 (844, 1024)\n",
      "463 (463, 1024)\n",
      "465 (465, 1024)\n",
      "291 (291, 1024)\n",
      "658 (658, 1024)\n",
      "362 (362, 1024)\n",
      "477 (477, 1024)\n",
      "579 (579, 1024)\n",
      "654 (654, 1024)\n",
      "280 (280, 1024)\n",
      "854 (854, 1024)\n",
      "725 (725, 1024)\n",
      "1233 (1233, 1024)\n",
      "347 (347, 1024)\n",
      "981 (981, 1024)\n",
      "1065 (1065, 1024)\n",
      "373 (373, 1024)\n",
      "1578 (1578, 1024)\n",
      "1841 (1841, 1024)\n",
      "1250 (1250, 1024)\n",
      "412 (412, 1024)\n",
      "1276 (1276, 1024)\n",
      "981 (981, 1024)\n",
      "1140 (1140, 1024)\n",
      "846 (846, 1024)\n",
      "1197 (1197, 1024)\n",
      "1551 (1551, 1024)\n",
      "1350 (1350, 1024)\n",
      "572 (572, 1024)\n",
      "282 (282, 1024)\n",
      "175 (175, 1024)\n",
      "395 (395, 1024)\n",
      "202 (202, 1024)\n",
      "672 (672, 1024)\n",
      "723 (723, 1024)\n",
      "181 (181, 1024)\n",
      "404 (404, 1024)\n",
      "312 (312, 1024)\n",
      "243 (243, 1024)\n",
      "142 (142, 1024)\n",
      "175 (175, 1024)\n",
      "211 (211, 1024)\n",
      "690 (690, 1024)\n",
      "636 (636, 1024)\n",
      "553 (553, 1024)\n",
      "792 (792, 1024)\n",
      "967 (967, 1024)\n",
      "867 (867, 1024)\n",
      "1564 (1564, 1024)\n",
      "1006 (1006, 1024)\n",
      "1618 (1618, 1024)\n",
      "1448 (1448, 1024)\n",
      "1384 (1384, 1024)\n",
      "377 (377, 1024)\n",
      "2143 (2143, 1024)\n",
      "2159 (2159, 1024)\n",
      "667 (667, 1024)\n",
      "1035 (1035, 1024)\n",
      "1000 (1000, 1024)\n",
      "482 (482, 1024)\n",
      "436 (436, 1024)\n",
      "521 (521, 1024)\n",
      "576 (576, 1024)\n",
      "551 (551, 1024)\n",
      "592 (592, 1024)\n",
      "294 (294, 1024)\n",
      "483 (483, 1024)\n",
      "630 (630, 1024)\n",
      "790 (790, 1024)\n",
      "1115 (1115, 1024)\n",
      "1113 (1113, 1024)\n",
      "1167 (1167, 1024)\n",
      "664 (664, 1024)\n",
      "753 (753, 1024)\n",
      "945 (945, 1024)\n",
      "303 (303, 1024)\n",
      "1078 (1078, 1024)\n",
      "841 (841, 1024)\n",
      "455 (455, 1024)\n",
      "450 (450, 1024)\n",
      "764 (764, 1024)\n",
      "1694 (1694, 1024)\n",
      "1558 (1558, 1024)\n",
      "699 (699, 1024)\n",
      "1306 (1306, 1024)\n",
      "235 (235, 1024)\n",
      "622 (622, 1024)\n",
      "379 (379, 1024)\n",
      "416 (416, 1024)\n",
      "762 (762, 1024)\n",
      "436 (436, 1024)\n",
      "402 (402, 1024)\n",
      "900 (900, 1024)\n",
      "605 (605, 1024)\n",
      "492 (492, 1024)\n",
      "603 (603, 1024)\n",
      "623 (623, 1024)\n",
      "271 (271, 1024)\n",
      "1064 (1064, 1024)\n",
      "839 (839, 1024)\n",
      "357 (357, 1024)\n",
      "400 (400, 1024)\n",
      "579 (579, 1024)\n",
      "663 (663, 1024)\n",
      "1083 (1083, 1024)\n",
      "1306 (1306, 1024)\n",
      "764 (764, 1024)\n",
      "509 (509, 1024)\n",
      "595 (595, 1024)\n",
      "720 (720, 1024)\n",
      "896 (896, 1024)\n",
      "1225 (1225, 1024)\n",
      "1001 (1001, 1024)\n",
      "793 (793, 1024)\n",
      "903 (903, 1024)\n",
      "349 (349, 1024)\n",
      "539 (539, 1024)\n",
      "712 (712, 1024)\n",
      "111 (111, 1024)\n",
      "177 (177, 1024)\n",
      "1843 (1843, 1024)\n",
      "1618 (1618, 1024)\n",
      "1351 (1351, 1024)\n",
      "98 (98, 1024)\n",
      "110 (110, 1024)\n",
      "146 (146, 1024)\n",
      "461 (461, 1024)\n",
      "394 (394, 1024)\n",
      "834 (834, 1024)\n",
      "938 (938, 1024)\n",
      "1097 (1097, 1024)\n",
      "1207 (1207, 1024)\n",
      "602 (602, 1024)\n",
      "842 (842, 1024)\n",
      "646 (646, 1024)\n",
      "620 (620, 1024)\n",
      "1243 (1243, 1024)\n",
      "1208 (1208, 1024)\n",
      "395 (395, 1024)\n",
      "400 (400, 1024)\n",
      "332 (332, 1024)\n",
      "457 (457, 1024)\n",
      "252 (252, 1024)\n",
      "253 (253, 1024)\n",
      "209 (209, 1024)\n",
      "169 (169, 1024)\n",
      "211 (211, 1024)\n",
      "243 (243, 1024)\n",
      "688 (688, 1024)\n",
      "856 (856, 1024)\n",
      "1251 (1251, 1024)\n",
      "1041 (1041, 1024)\n",
      "1491 (1491, 1024)\n",
      "1533 (1533, 1024)\n",
      "837 (837, 1024)\n",
      "804 (804, 1024)\n",
      "1031 (1031, 1024)\n",
      "451 (451, 1024)\n",
      "371 (371, 1024)\n",
      "645 (645, 1024)\n",
      "176 (176, 1024)\n",
      "712 (712, 1024)\n",
      "523 (523, 1024)\n",
      "293 (293, 1024)\n",
      "439 (439, 1024)\n",
      "334 (334, 1024)\n",
      "375 (375, 1024)\n",
      "276 (276, 1024)\n",
      "391 (391, 1024)\n",
      "1318 (1318, 1024)\n",
      "1652 (1652, 1024)\n",
      "1692 (1692, 1024)\n",
      "1433 (1433, 1024)\n",
      "1146 (1146, 1024)\n",
      "1866 (1866, 1024)\n",
      "1767 (1767, 1024)\n",
      "606 (606, 1024)\n",
      "953 (953, 1024)\n",
      "840 (840, 1024)\n",
      "1200 (1200, 1024)\n",
      "832 (832, 1024)\n",
      "694 (694, 1024)\n",
      "1857 (1857, 1024)\n",
      "1599 (1599, 1024)\n",
      "900 (900, 1024)\n",
      "2008 (2008, 1024)\n",
      "833 (833, 1024)\n",
      "418 (418, 1024)\n",
      "866 (866, 1024)\n",
      "560 (560, 1024)\n",
      "653 (653, 1024)\n",
      "756 (756, 1024)\n",
      "452 (452, 1024)\n",
      "769 (769, 1024)\n",
      "800 (800, 1024)\n",
      "464 (464, 1024)\n",
      "500 (500, 1024)\n",
      "275 (275, 1024)\n",
      "275 (275, 1024)\n",
      "286 (286, 1024)\n",
      "282 (282, 1024)\n",
      "324 (324, 1024)\n",
      "323 (323, 1024)\n",
      "259 (259, 1024)\n",
      "301 (301, 1024)\n",
      "343 (343, 1024)\n",
      "208 (208, 1024)\n",
      "293 (293, 1024)\n",
      "427 (427, 1024)\n",
      "1066 (1066, 1024)\n",
      "424 (424, 1024)\n",
      "338 (338, 1024)\n",
      "410 (410, 1024)\n",
      "398 (398, 1024)\n",
      "520 (520, 1024)\n",
      "428 (428, 1024)\n",
      "862 (862, 1024)\n",
      "787 (787, 1024)\n",
      "676 (676, 1024)\n",
      "565 (565, 1024)\n",
      "478 (478, 1024)\n",
      "758 (758, 1024)\n",
      "431 (431, 1024)\n",
      "450 (450, 1024)\n",
      "777 (777, 1024)\n",
      "678 (678, 1024)\n",
      "479 (479, 1024)\n",
      "762 (762, 1024)\n",
      "275 (275, 1024)\n",
      "289 (289, 1024)\n",
      "242 (242, 1024)\n",
      "245 (245, 1024)\n",
      "246 (246, 1024)\n",
      "171 (171, 1024)\n",
      "297 (297, 1024)\n",
      "409 (409, 1024)\n",
      "258 (258, 1024)\n",
      "387 (387, 1024)\n",
      "391 (391, 1024)\n",
      "301 (301, 1024)\n",
      "336 (336, 1024)\n",
      "401 (401, 1024)\n",
      "210 (210, 1024)\n",
      "587 (587, 1024)\n",
      "424 (424, 1024)\n",
      "394 (394, 1024)\n",
      "169 (169, 1024)\n",
      "390 (390, 1024)\n",
      "148 (148, 1024)\n",
      "562 (562, 1024)\n",
      "860 (860, 1024)\n",
      "580 (580, 1024)\n",
      "431 (431, 1024)\n",
      "281 (281, 1024)\n",
      "372 (372, 1024)\n",
      "732 (732, 1024)\n",
      "363 (363, 1024)\n",
      "476 (476, 1024)\n",
      "486 (486, 1024)\n",
      "505 (505, 1024)\n",
      "366 (366, 1024)\n",
      "239 (239, 1024)\n",
      "300 (300, 1024)\n",
      "321 (321, 1024)\n",
      "1313 (1313, 1024)\n",
      "1011 (1011, 1024)\n",
      "1178 (1178, 1024)\n",
      "1649 (1649, 1024)\n",
      "612 (612, 1024)\n",
      "498 (498, 1024)\n",
      "456 (456, 1024)\n",
      "445 (445, 1024)\n",
      "447 (447, 1024)\n",
      "668 (668, 1024)\n",
      "1647 (1647, 1024)\n",
      "784 (784, 1024)\n",
      "1032 (1032, 1024)\n",
      "928 (928, 1024)\n"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "videoname2audio_features_sampled={}\n",
    "for videoname in videoname2img_names:\n",
    "    audio_embs=videoname2audio_features[videoname]\n",
    "    x = np.linspace(0, 1, audio_embs.shape[0])\n",
    "    f = interp1d(x, audio_embs,axis=0)\n",
    "    new_x=np.linspace(0, 1, len(videoname2img_names[videoname]))\n",
    "    videoname2audio_features_sampled[videoname]=f(new_x)\n",
    "    print(len(videoname2img_names[videoname]),videoname2audio_features_sampled[videoname].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f1e88e53e5421eb8867c98f3d6bb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "videoname2audio_features_test={}\n",
    "for subject in tqdm(sorted(os.listdir(TEST_VIDEO_DIR))):\n",
    "    subject_dir=os.path.join(TEST_VIDEO_DIR,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)\n",
    "        audio_dir=TEST_AUDIO_DIR+subject+'/'+visit_name+'/'\n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            videoname=subject+'/'+visit_name+'/'+question_name\n",
    "            waveFile=audio_dir+question_name+'.wav'\n",
    "            array, fs = torchaudio.load(waveFile)\n",
    "            inp = processor(array.squeeze(), sampling_rate=fs, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inp)\n",
    "            videoname2audio_features_test[videoname]=outputs.last_hidden_state.numpy()[0]\n",
    "    #print(videoname2audio_features_test[videoname].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n"
     ]
    }
   ],
   "source": [
    "#MODEL2AUDIOFEATURES_TEST='test_bah_wav2vec2.pickle'\n",
    "MODEL2AUDIOFEATURES_TEST='test_bah_hubert.pickle'\n",
    "\n",
    "if False:\n",
    "    with open(MODEL2AUDIOFEATURES_TEST, 'wb') as handle:\n",
    "        pickle.dump(videoname2audio_features_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(MODEL2AUDIOFEATURES_TEST, 'rb') as handle:\n",
    "        videoname2audio_features_test=pickle.load(handle)\n",
    "print(len(videoname2audio_features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 (418, 1024)\n",
      "421 (421, 1024)\n",
      "348 (348, 1024)\n",
      "368 (368, 1024)\n",
      "248 (248, 1024)\n",
      "475 (475, 1024)\n",
      "349 (349, 1024)\n",
      "938 (938, 1024)\n",
      "1220 (1220, 1024)\n",
      "1238 (1238, 1024)\n",
      "1053 (1053, 1024)\n",
      "884 (884, 1024)\n",
      "1765 (1765, 1024)\n",
      "1064 (1064, 1024)\n",
      "855 (855, 1024)\n",
      "502 (502, 1024)\n",
      "488 (488, 1024)\n",
      "630 (630, 1024)\n",
      "429 (429, 1024)\n",
      "558 (558, 1024)\n",
      "420 (420, 1024)\n",
      "467 (467, 1024)\n",
      "985 (985, 1024)\n",
      "1119 (1119, 1024)\n",
      "827 (827, 1024)\n",
      "1267 (1267, 1024)\n",
      "635 (635, 1024)\n",
      "1118 (1118, 1024)\n",
      "439 (439, 1024)\n",
      "873 (873, 1024)\n",
      "816 (816, 1024)\n",
      "1079 (1079, 1024)\n",
      "361 (361, 1024)\n",
      "420 (420, 1024)\n",
      "600 (600, 1024)\n",
      "451 (451, 1024)\n",
      "341 (341, 1024)\n",
      "255 (255, 1024)\n",
      "384 (384, 1024)\n",
      "285 (285, 1024)\n",
      "181 (181, 1024)\n",
      "270 (270, 1024)\n",
      "371 (371, 1024)\n",
      "320 (320, 1024)\n",
      "396 (396, 1024)\n",
      "421 (421, 1024)\n",
      "298 (298, 1024)\n",
      "438 (438, 1024)\n",
      "409 (409, 1024)\n",
      "249 (249, 1024)\n",
      "386 (386, 1024)\n",
      "207 (207, 1024)\n",
      "248 (248, 1024)\n",
      "158 (158, 1024)\n",
      "254 (254, 1024)\n",
      "346 (346, 1024)\n",
      "411 (411, 1024)\n",
      "569 (569, 1024)\n",
      "498 (498, 1024)\n",
      "440 (440, 1024)\n",
      "394 (394, 1024)\n",
      "672 (672, 1024)\n",
      "354 (354, 1024)\n",
      "505 (505, 1024)\n",
      "165 (165, 1024)\n",
      "205 (205, 1024)\n",
      "410 (410, 1024)\n",
      "344 (344, 1024)\n",
      "242 (242, 1024)\n",
      "252 (252, 1024)\n",
      "162 (162, 1024)\n",
      "204 (204, 1024)\n",
      "130 (130, 1024)\n",
      "177 (177, 1024)\n",
      "173 (173, 1024)\n",
      "144 (144, 1024)\n",
      "160 (160, 1024)\n",
      "391 (391, 1024)\n",
      "500 (500, 1024)\n",
      "339 (339, 1024)\n",
      "298 (298, 1024)\n",
      "323 (323, 1024)\n",
      "340 (340, 1024)\n",
      "340 (340, 1024)\n",
      "676 (676, 1024)\n",
      "552 (552, 1024)\n",
      "982 (982, 1024)\n",
      "1044 (1044, 1024)\n",
      "817 (817, 1024)\n",
      "1087 (1087, 1024)\n",
      "962 (962, 1024)\n",
      "459 (459, 1024)\n",
      "263 (263, 1024)\n",
      "299 (299, 1024)\n",
      "418 (418, 1024)\n",
      "234 (234, 1024)\n",
      "205 (205, 1024)\n",
      "185 (185, 1024)\n",
      "696 (696, 1024)\n",
      "382 (382, 1024)\n",
      "675 (675, 1024)\n",
      "454 (454, 1024)\n",
      "322 (322, 1024)\n",
      "387 (387, 1024)\n",
      "385 (385, 1024)\n",
      "307 (307, 1024)\n",
      "586 (586, 1024)\n",
      "608 (608, 1024)\n",
      "406 (406, 1024)\n",
      "388 (388, 1024)\n",
      "481 (481, 1024)\n",
      "752 (752, 1024)\n",
      "577 (577, 1024)\n",
      "567 (567, 1024)\n",
      "411 (411, 1024)\n",
      "445 (445, 1024)\n",
      "657 (657, 1024)\n",
      "401 (401, 1024)\n",
      "713 (713, 1024)\n",
      "1136 (1136, 1024)\n",
      "1460 (1460, 1024)\n",
      "1350 (1350, 1024)\n",
      "1375 (1375, 1024)\n",
      "1266 (1266, 1024)\n",
      "1731 (1731, 1024)\n",
      "1718 (1718, 1024)\n",
      "200 (200, 1024)\n",
      "146 (146, 1024)\n",
      "274 (274, 1024)\n",
      "202 (202, 1024)\n",
      "313 (313, 1024)\n",
      "252 (252, 1024)\n",
      "298 (298, 1024)\n",
      "397 (397, 1024)\n",
      "477 (477, 1024)\n",
      "433 (433, 1024)\n",
      "459 (459, 1024)\n",
      "340 (340, 1024)\n",
      "343 (343, 1024)\n",
      "516 (516, 1024)\n",
      "266 (266, 1024)\n",
      "397 (397, 1024)\n",
      "350 (350, 1024)\n",
      "761 (761, 1024)\n",
      "271 (271, 1024)\n",
      "472 (472, 1024)\n",
      "595 (595, 1024)\n",
      "863 (863, 1024)\n",
      "1273 (1273, 1024)\n",
      "1576 (1576, 1024)\n",
      "768 (768, 1024)\n",
      "954 (954, 1024)\n",
      "693 (693, 1024)\n",
      "814 (814, 1024)\n",
      "667 (667, 1024)\n",
      "920 (920, 1024)\n",
      "980 (980, 1024)\n",
      "950 (950, 1024)\n",
      "940 (940, 1024)\n",
      "981 (981, 1024)\n",
      "545 (545, 1024)\n",
      "207 (207, 1024)\n",
      "264 (264, 1024)\n",
      "334 (334, 1024)\n",
      "273 (273, 1024)\n",
      "232 (232, 1024)\n",
      "121 (121, 1024)\n",
      "328 (328, 1024)\n",
      "512 (512, 1024)\n",
      "358 (358, 1024)\n",
      "561 (561, 1024)\n",
      "283 (283, 1024)\n",
      "411 (411, 1024)\n",
      "261 (261, 1024)\n",
      "299 (299, 1024)\n",
      "436 (436, 1024)\n",
      "434 (434, 1024)\n",
      "374 (374, 1024)\n",
      "274 (274, 1024)\n",
      "581 (581, 1024)\n",
      "546 (546, 1024)\n",
      "355 (355, 1024)\n",
      "742 (742, 1024)\n",
      "1272 (1272, 1024)\n",
      "1030 (1030, 1024)\n",
      "1136 (1136, 1024)\n",
      "1128 (1128, 1024)\n",
      "719 (719, 1024)\n",
      "1437 (1437, 1024)\n",
      "703 (703, 1024)\n",
      "1599 (1599, 1024)\n",
      "1286 (1286, 1024)\n",
      "1188 (1188, 1024)\n",
      "801 (801, 1024)\n",
      "1114 (1114, 1024)\n",
      "1376 (1376, 1024)\n",
      "556 (556, 1024)\n",
      "735 (735, 1024)\n",
      "865 (865, 1024)\n",
      "706 (706, 1024)\n",
      "623 (623, 1024)\n",
      "533 (533, 1024)\n",
      "583 (583, 1024)\n",
      "308 (308, 1024)\n",
      "313 (313, 1024)\n",
      "644 (644, 1024)\n",
      "323 (323, 1024)\n",
      "326 (326, 1024)\n",
      "334 (334, 1024)\n",
      "427 (427, 1024)\n",
      "227 (227, 1024)\n",
      "551 (551, 1024)\n",
      "609 (609, 1024)\n",
      "550 (550, 1024)\n",
      "343 (343, 1024)\n",
      "682 (682, 1024)\n",
      "641 (641, 1024)\n",
      "434 (434, 1024)\n",
      "340 (340, 1024)\n",
      "280 (280, 1024)\n",
      "533 (533, 1024)\n",
      "491 (491, 1024)\n",
      "412 (412, 1024)\n",
      "312 (312, 1024)\n",
      "461 (461, 1024)\n",
      "671 (671, 1024)\n",
      "1066 (1066, 1024)\n",
      "480 (480, 1024)\n",
      "384 (384, 1024)\n",
      "825 (825, 1024)\n",
      "615 (615, 1024)\n",
      "143 (143, 1024)\n",
      "168 (168, 1024)\n",
      "184 (184, 1024)\n",
      "199 (199, 1024)\n",
      "146 (146, 1024)\n",
      "188 (188, 1024)\n",
      "182 (182, 1024)\n",
      "667 (667, 1024)\n",
      "949 (949, 1024)\n",
      "836 (836, 1024)\n",
      "1049 (1049, 1024)\n",
      "1087 (1087, 1024)\n",
      "1266 (1266, 1024)\n",
      "682 (682, 1024)\n",
      "451 (451, 1024)\n",
      "446 (446, 1024)\n",
      "563 (563, 1024)\n",
      "280 (280, 1024)\n",
      "630 (630, 1024)\n",
      "877 (877, 1024)\n",
      "820 (820, 1024)\n",
      "255 (255, 1024)\n",
      "525 (525, 1024)\n",
      "489 (489, 1024)\n",
      "367 (367, 1024)\n",
      "314 (314, 1024)\n",
      "228 (228, 1024)\n",
      "291 (291, 1024)\n",
      "434 (434, 1024)\n",
      "898 (898, 1024)\n",
      "1235 (1235, 1024)\n",
      "1561 (1561, 1024)\n",
      "664 (664, 1024)\n",
      "945 (945, 1024)\n",
      "739 (739, 1024)\n",
      "801 (801, 1024)\n",
      "981 (981, 1024)\n",
      "1166 (1166, 1024)\n",
      "1264 (1264, 1024)\n",
      "785 (785, 1024)\n",
      "1238 (1238, 1024)\n",
      "1072 (1072, 1024)\n",
      "643 (643, 1024)\n",
      "1763 (1763, 1024)\n",
      "2295 (2295, 1024)\n",
      "1144 (1144, 1024)\n",
      "1750 (1750, 1024)\n",
      "560 (560, 1024)\n",
      "1109 (1109, 1024)\n"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "videoname2audio_features_sampled_test={}\n",
    "for videoname in videoname2img_names_test:\n",
    "    audio_embs=videoname2audio_features_test[videoname]\n",
    "    x = np.linspace(0, 1, audio_embs.shape[0])\n",
    "    f = interp1d(x, audio_embs,axis=0)\n",
    "    new_x=np.linspace(0, 1, len(videoname2img_names_test[videoname]))\n",
    "    videoname2audio_features_sampled_test[videoname]=f(new_x)\n",
    "    print(len(videoname2img_names_test[videoname]),videoname2audio_features_sampled_test[videoname].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]<ipython-input-37-8b0f0115b24f>:11: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  transcr = yaml.load(f)\n",
      "100%|██████████| 84/84 [00:01<00:00, 47.39it/s]\n"
     ]
    }
   ],
   "source": [
    "videoname2text={}\n",
    "for subject in tqdm(sorted(os.listdir(TRANSCRIPTION_DIR))):\n",
    "    subject_dir=os.path.join(TRANSCRIPTION_DIR,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)   \n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            question_dir=os.path.join(visit_dir,question_name)   \n",
    "            \n",
    "            videoname=subject+'/'+visit_name+'/'+question_name\n",
    "            with open(question_dir+'/'+question_name.replace('.mp4','.yml'), 'r') as f:\n",
    "                transcr = yaml.load(f)\n",
    "\n",
    "            videoname2text[videoname]=transcr['text']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431  I usually get out of bed, then brush my teeth, then have breakfast with coffee.\n"
     ]
    }
   ],
   "source": [
    "AUDIO2TEXTS='bah_text.pickle'\n",
    "\n",
    "if False:\n",
    "    with open(AUDIO2TEXTS, 'wb') as handle:\n",
    "        pickle.dump(videoname2text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(AUDIO2TEXTS, 'rb') as handle:\n",
    "        videoname2text=pickle.load(handle)\n",
    "print(len(videoname2text),videoname2text['82596/Visite_1/82596_Question_1_2024-10-30_21-43-58_Video.mp4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]<ipython-input-6-d281e465ee23>:11: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  transcr = yaml.load(f)\n",
      "100%|██████████| 40/40 [00:03<00:00, 12.80it/s]\n"
     ]
    }
   ],
   "source": [
    "videoname2text_test={}\n",
    "for subject in tqdm(sorted(os.listdir(TEST_TRANSCRIPTION_DIR))):\n",
    "    subject_dir=os.path.join(TEST_TRANSCRIPTION_DIR,subject)   \n",
    "    for visit_name in sorted(os.listdir(subject_dir)):\n",
    "        visit_dir=os.path.join(subject_dir,visit_name)   \n",
    "        for question_name in sorted(os.listdir(visit_dir)):\n",
    "            question_dir=os.path.join(visit_dir,question_name)   \n",
    "            \n",
    "            videoname=subject+'/'+visit_name+'/'+question_name\n",
    "            with open(question_dir+'/'+question_name.replace('.mp4','.yml'), 'r') as f:\n",
    "                transcr = yaml.load(f)\n",
    "\n",
    "            videoname2text_test[videoname]=transcr['text']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n"
     ]
    }
   ],
   "source": [
    "AUDIO2TEXTS_TEST='test_bah_text.pickle'\n",
    "\n",
    "if False:\n",
    "    with open(AUDIO2TEXTS_TEST, 'wb') as handle:\n",
    "        pickle.dump(videoname2text_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(AUDIO2TEXTS_TEST, 'rb') as handle:\n",
    "        videoname2text_test=pickle.load(handle)\n",
    "print(len(videoname2text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_path=\"SamLowe/roberta-base-go_emotions\"\n",
    "classifier = pipeline(task=\"feature-extraction\", model=model_path, tokenizer=model_path, max_length=512, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431/431 [00:19<00:00, 21.83it/s]\n"
     ]
    }
   ],
   "source": [
    "text_embeddings_list=[]\n",
    "sentences=[]\n",
    "for videoname in tqdm(videoname2text):\n",
    "    sentences.append(videoname2text[videoname])\n",
    "    if len(sentences)>32:\n",
    "        outputs=classifier(sentences, return_tensors=\"pt\")\n",
    "        for output in outputs:\n",
    "            text_embeddings_list.append(output[0].numpy())\n",
    "        sentences=[]\n",
    "        #break\n",
    "if len(sentences)>0:\n",
    "    outputs=classifier(sentences, return_tensors=\"pt\")\n",
    "    for output in outputs:\n",
    "        text_embeddings_list.append(output[0].numpy())\n",
    "    sentences=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#model_name='distilroberta-base'\n",
    "model_name='all-MiniLM-L6-v1'\n",
    "#model_name='intfloat/e5-base-v2'\n",
    "model_st = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431/431 [00:00<00:00, 1094.00it/s]\n"
     ]
    }
   ],
   "source": [
    "text_embeddings_list=[]\n",
    "sentences=[]\n",
    "for videoname in tqdm(videoname2text):\n",
    "    sentences.append(videoname2text[videoname])\n",
    "    if len(sentences)>32:\n",
    "        outputs=model_st.encode(sentences)\n",
    "        for output in outputs:\n",
    "            text_embeddings_list.append(output)\n",
    "        sentences=[]\n",
    "        #break\n",
    "if len(sentences)>0:\n",
    "    outputs=model_st.encode(sentences)\n",
    "    for output in outputs:\n",
    "        text_embeddings_list.append(output)\n",
    "    sentences=[]\n",
    "#print(text_embeddings_list[0].shape,text_embeddings_list[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_embeddings_list[2].shape\n",
    "text_embeddings={}\n",
    "for i,videoname in enumerate(videoname2text):\n",
    "    text_embeddings[videoname]=text_embeddings_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 (20, 768)\n"
     ]
    }
   ],
   "source": [
    "AUDIO2TEXT_EMBEDDINGS='bah_text_roberta-base-go_emotions.pickle'\n",
    "#AUDIO2TEXT_EMBEDDINGS='bah_text_openai_small.pickle'\n",
    "#AUDIO2TEXT_EMBEDDINGS='bah_text_gigachat.pickle'\n",
    "\n",
    "if False:\n",
    "    with open(AUDIO2TEXT_EMBEDDINGS, 'wb') as handle:\n",
    "        pickle.dump(text_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(AUDIO2TEXT_EMBEDDINGS, 'rb') as handle:\n",
    "        text_embeddings=pickle.load(handle)\n",
    "print(len(text_embeddings),text_embeddings['82596/Visite_1/82596_Question_1_2024-10-30_21-43-58_Video.mp4'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1318 1318 (191, 768) 6.900523560209424 (2740, 1024) 2.078907435508346\n",
      "1652 1652 (228, 768) 7.245614035087719 (3439, 1024) 2.081719128329298\n",
      "1692 1692 (272, 768) 6.220588235294118 (3519, 1024) 2.0797872340425534\n",
      "1433 1433 (207, 768) 6.9227053140096615 (2982, 1024) 2.0809490579204466\n",
      "1146 1146 (207, 768) 5.536231884057971 (2382, 1024) 2.0785340314136125\n",
      "1866 1866 (287, 768) 6.501742160278746 (3856, 1024) 2.0664523043944265\n",
      "1767 1767 (314, 768) 5.627388535031847 (3676, 1024) 2.080362195812111\n",
      "197 197 (20, 768) 9.85 (411, 1024) 2.0862944162436547\n",
      "315 315 (38, 768) 8.289473684210526 (654, 1024) 2.0761904761904764\n",
      "450 450 (50, 768) 9.0 (936, 1024) 2.08\n",
      "122 122 (13, 768) 9.384615384615385 (256, 1024) 2.098360655737705\n",
      "222 222 (24, 768) 9.25 (462, 1024) 2.081081081081081\n",
      "262 262 (29, 768) 9.03448275862069 (545, 1024) 2.0801526717557253\n",
      "141 141 (16, 768) 8.8125 (291, 1024) 2.0638297872340425\n",
      "720 720 (92, 768) 7.826086956521739 (1503, 1024) 2.0875\n",
      "896 896 (144, 768) 6.222222222222222 (1866, 1024) 2.0825892857142856\n",
      "1225 1225 (193, 768) 6.347150259067358 (2550, 1024) 2.0816326530612246\n",
      "1001 1001 (140, 768) 7.15 (2088, 1024) 2.085914085914086\n",
      "793 793 (136, 768) 5.830882352941177 (1656, 1024) 2.0882723833543504\n",
      "903 903 (164, 768) 5.5060975609756095 (1884, 1024) 2.086378737541528\n",
      "690 690 (68, 768) 10.147058823529411 (1437, 1024) 2.082608695652174\n",
      "636 636 (80, 768) 7.95 (1327, 1024) 2.0864779874213837\n",
      "553 553 (78, 768) 7.089743589743589 (1152, 1024) 2.0831826401446656\n",
      "792 792 (84, 768) 9.428571428571429 (1650, 1024) 2.0833333333333335\n",
      "967 967 (117, 768) 8.264957264957266 (2016, 1024) 2.0847983453981387\n",
      "694 694 (77, 768) 9.012987012987013 (1444, 1024) 2.080691642651297\n",
      "1857 1857 (266, 768) 6.981203007518797 (3865, 1024) 2.0813139472267097\n",
      "1599 1599 (198, 768) 8.075757575757576 (3328, 1024) 2.0813008130081303\n",
      "900 900 (117, 768) 7.6923076923076925 (1872, 1024) 2.08\n",
      "2008 2008 (202, 768) 9.94059405940594 (4183, 1024) 2.083167330677291\n",
      "331 331 (23, 768) 14.391304347826088 (690, 1024) 2.084592145015106\n",
      "659 659 (67, 768) 9.835820895522389 (1371, 1024) 2.080424886191199\n",
      "761 761 (78, 768) 9.756410256410257 (1587, 1024) 2.085413929040736\n",
      "672 672 (57, 768) 11.789473684210526 (1399, 1024) 2.081845238095238\n",
      "1236 1236 (57, 768) 21.68421052631579 (2575, 1024) 2.0833333333333335\n",
      "858 858 (85, 768) 10.094117647058823 (1788, 1024) 2.0839160839160837\n",
      "1121 1121 (120, 768) 9.341666666666667 (2336, 1024) 2.0838537020517394\n",
      "646 646 (49, 768) 13.183673469387756 (1343, 1024) 2.0789473684210527\n",
      "409 409 (55, 768) 7.4363636363636365 (852, 1024) 2.0831295843520783\n",
      "258 258 (36, 768) 7.166666666666667 (537, 1024) 2.0813953488372094\n",
      "387 387 (41, 768) 9.439024390243903 (807, 1024) 2.0852713178294575\n",
      "391 391 (43, 768) 9.093023255813954 (815, 1024) 2.084398976982097\n",
      "301 301 (33, 768) 9.121212121212121 (626, 1024) 2.079734219269103\n",
      "336 336 (38, 768) 8.842105263157896 (702, 1024) 2.0892857142857144\n",
      "401 401 (49, 768) 8.183673469387756 (837, 1024) 2.0872817955112217\n",
      "247 247 (24, 768) 10.291666666666666 (516, 1024) 2.0890688259109313\n",
      "147 147 (18, 768) 8.166666666666666 (306, 1024) 2.0816326530612246\n",
      "283 283 (38, 768) 7.447368421052632 (592, 1024) 2.091872791519435\n",
      "213 213 (25, 768) 8.52 (445, 1024) 2.0892018779342725\n",
      "215 215 (29, 768) 7.413793103448276 (448, 1024) 2.083720930232558\n",
      "307 307 (53, 768) 5.7924528301886795 (640, 1024) 2.0846905537459284\n",
      "717 717 (81, 768) 8.851851851851851 (1494, 1024) 2.083682008368201\n",
      "645 645 (66, 768) 9.772727272727273 (1342, 1024) 2.0806201550387597\n",
      "723 723 (83, 768) 8.710843373493976 (1506, 1024) 2.08298755186722\n",
      "876 876 (96, 768) 9.125 (1824, 1024) 2.0821917808219177\n",
      "721 721 (70, 768) 10.3 (1501, 1024) 2.0818307905686546\n",
      "845 845 (107, 768) 7.897196261682243 (1761, 1024) 2.084023668639053\n",
      "844 844 (77, 768) 10.96103896103896 (1759, 1024) 2.0841232227488153\n",
      "210 210 (6, 768) 35.0 (438, 1024) 2.085714285714286\n",
      "587 587 (70, 768) 8.385714285714286 (1225, 1024) 2.0868824531516186\n",
      "424 424 (51, 768) 8.313725490196079 (882, 1024) 2.080188679245283\n",
      "394 394 (43, 768) 9.162790697674419 (819, 1024) 2.0786802030456855\n",
      "169 169 (13, 768) 13.0 (352, 1024) 2.0828402366863905\n",
      "390 390 (12, 768) 32.5 (811, 1024) 2.0794871794871796\n",
      "148 148 (17, 768) 8.705882352941176 (306, 1024) 2.0675675675675675\n",
      "1066 1066 (95, 768) 11.221052631578948 (2223, 1024) 2.0853658536585367\n",
      "688 688 (38, 768) 18.105263157894736 (1434, 1024) 2.0843023255813953\n",
      "856 856 (127, 768) 6.74015748031496 (1783, 1024) 2.082943925233645\n",
      "994 994 (95, 768) 10.463157894736842 (2068, 1024) 2.080482897384306\n",
      "1092 1092 (144, 768) 7.583333333333333 (2274, 1024) 2.0824175824175826\n",
      "1055 1055 (144, 768) 7.326388888888889 (2197, 1024) 2.082464454976303\n",
      "790 790 (93, 768) 8.494623655913978 (1642, 1024) 2.078481012658228\n",
      "1376 1376 (163, 768) 8.441717791411042 (2865, 1024) 2.082122093023256\n",
      "953 953 (147, 768) 6.482993197278912 (1985, 1024) 2.08289611752361\n",
      "840 840 (134, 768) 6.268656716417911 (1749, 1024) 2.0821428571428573\n",
      "1200 1200 (161, 768) 7.453416149068323 (2499, 1024) 2.0825\n",
      "832 832 (131, 768) 6.351145038167939 (1732, 1024) 2.081730769230769\n",
      "412 412 (37, 768) 11.135135135135135 (864, 1024) 2.0970873786407767\n",
      "1276 1276 (107, 768) 11.925233644859814 (2658, 1024) 2.08307210031348\n",
      "349 349 (29, 768) 12.03448275862069 (729, 1024) 2.0888252148997135\n",
      "539 539 (42, 768) 12.833333333333334 (1108, 1024) 2.0556586270871984\n",
      "712 712 (70, 768) 10.17142857142857 (1483, 1024) 2.082865168539326\n",
      "930 930 (53, 768) 17.547169811320753 (1938, 1024) 2.0838709677419356\n",
      "504 504 (37, 768) 13.621621621621621 (1050, 1024) 2.0833333333333335\n",
      "914 914 (80, 768) 11.425 (1905, 1024) 2.0842450765864333\n",
      "922 922 (71, 768) 12.985915492957746 (1920, 1024) 2.0824295010845986\n",
      "327 327 (23, 768) 14.217391304347826 (682, 1024) 2.085626911314985\n",
      "524 524 (43, 768) 12.186046511627907 (1092, 1024) 2.0839694656488548\n",
      "925 925 (76, 768) 12.171052631578947 (1927, 1024) 2.0832432432432433\n",
      "476 476 (36, 768) 13.222222222222221 (991, 1024) 2.081932773109244\n",
      "486 486 (60, 768) 8.1 (1014, 1024) 2.0864197530864197\n",
      "505 505 (56, 768) 9.017857142857142 (1053, 1024) 2.0851485148514852\n",
      "366 366 (30, 768) 12.2 (760, 1024) 2.0765027322404372\n",
      "239 239 (33, 768) 7.242424242424242 (501, 1024) 2.096234309623431\n",
      "300 300 (24, 768) 12.5 (624, 1024) 2.08\n",
      "576 576 (87, 768) 6.620689655172414 (1200, 1024) 2.0833333333333335\n",
      "551 551 (78, 768) 7.064102564102564 (1150, 1024) 2.087114337568058\n",
      "592 592 (55, 768) 10.763636363636364 (1236, 1024) 2.0878378378378377\n",
      "294 294 (37, 768) 7.945945945945946 (612, 1024) 2.0816326530612246\n",
      "483 483 (69, 768) 7.0 (1008, 1024) 2.0869565217391304\n",
      "630 630 (88, 768) 7.159090909090909 (1314, 1024) 2.085714285714286\n",
      "836 836 (67, 768) 12.477611940298507 (1740, 1024) 2.0813397129186604\n",
      "862 862 (84, 768) 10.261904761904763 (1793, 1024) 2.0800464037122968\n",
      "768 768 (84, 768) 9.142857142857142 (1599, 1024) 2.08203125\n",
      "446 446 (40, 768) 11.15 (927, 1024) 2.07847533632287\n",
      "641 641 (53, 768) 12.09433962264151 (1335, 1024) 2.0826833073322932\n",
      "218 218 (11, 768) 19.818181818181817 (453, 1024) 2.0779816513761467\n",
      "394 394 (41, 768) 9.609756097560975 (818, 1024) 2.0761421319796955\n",
      "804 804 (83, 768) 9.686746987951807 (1672, 1024) 2.0796019900497513\n",
      "1302 1302 (134, 768) 9.716417910447761 (2706, 1024) 2.078341013824885\n",
      "950 950 (99, 768) 9.595959595959595 (1972, 1024) 2.0757894736842104\n",
      "1288 1288 (131, 768) 9.83206106870229 (2686, 1024) 2.0854037267080745\n",
      "927 927 (81, 768) 11.444444444444445 (1930, 1024) 2.081984897518878\n",
      "1509 1509 (118, 768) 12.788135593220339 (3147, 1024) 2.085487077534791\n",
      "1373 1373 (118, 768) 11.635593220338983 (2854, 1024) 2.0786598689002185\n",
      "389 389 (29, 768) 13.413793103448276 (811, 1024) 2.0848329048843186\n",
      "646 646 (63, 768) 10.253968253968255 (1344, 1024) 2.0804953560371517\n",
      "420 420 (44, 768) 9.545454545454545 (874, 1024) 2.080952380952381\n",
      "465 465 (58, 768) 8.017241379310345 (967, 1024) 2.0795698924731183\n",
      "370 370 (44, 768) 8.409090909090908 (772, 1024) 2.0864864864864865\n",
      "433 433 (43, 768) 10.069767441860465 (901, 1024) 2.0808314087759814\n",
      "339 339 (46, 768) 7.369565217391305 (705, 1024) 2.079646017699115\n",
      "111 111 (5, 768) 22.2 (234, 1024) 2.108108108108108\n",
      "498 498 (47, 768) 10.595744680851064 (1036, 1024) 2.0803212851405624\n",
      "456 456 (47, 768) 9.702127659574469 (949, 1024) 2.081140350877193\n",
      "445 445 (36, 768) 12.36111111111111 (928, 1024) 2.0853932584269663\n",
      "447 447 (38, 768) 11.763157894736842 (930, 1024) 2.0805369127516777\n"
     ]
    }
   ],
   "source": [
    "for videoname in videoname2AH_val:\n",
    "    frame2ind={}\n",
    "    for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "        frame2ind[frame]=ind\n",
    "    text_embs=text_embeddings[videoname]\n",
    "    audio_embs=videoname2audio_features[videoname]\n",
    "    print(len(videoname2img_names[videoname]),len(frame2ind),text_embs.shape,len(frame2ind)/text_embs.shape[0],audio_embs.shape,audio_embs.shape[0]/len(frame2ind))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 (482, 768)\n",
      "730 (730, 768)\n",
      "835 (835, 768)\n",
      "610 (610, 768)\n",
      "681 (681, 768)\n",
      "1115 (1115, 768)\n",
      "721 (721, 768)\n",
      "389 (389, 768)\n",
      "646 (646, 768)\n",
      "420 (420, 768)\n",
      "465 (465, 768)\n",
      "370 (370, 768)\n",
      "433 (433, 768)\n",
      "339 (339, 768)\n",
      "349 (349, 768)\n",
      "477 (477, 768)\n",
      "801 (801, 768)\n",
      "572 (572, 768)\n",
      "290 (290, 768)\n",
      "306 (306, 768)\n",
      "285 (285, 768)\n",
      "320 (320, 768)\n",
      "467 (467, 768)\n",
      "417 (417, 768)\n",
      "513 (513, 768)\n",
      "610 (610, 768)\n",
      "546 (546, 768)\n",
      "554 (554, 768)\n",
      "331 (331, 768)\n",
      "659 (659, 768)\n",
      "761 (761, 768)\n",
      "672 (672, 768)\n",
      "1236 (1236, 768)\n",
      "858 (858, 768)\n",
      "1121 (1121, 768)\n",
      "806 (806, 768)\n",
      "1408 (1408, 768)\n",
      "953 (953, 768)\n",
      "930 (930, 768)\n",
      "908 (908, 768)\n",
      "801 (801, 768)\n",
      "1391 (1391, 768)\n",
      "748 (748, 768)\n",
      "493 (493, 768)\n",
      "561 (561, 768)\n",
      "484 (484, 768)\n",
      "384 (384, 768)\n",
      "399 (399, 768)\n",
      "930 (930, 768)\n",
      "504 (504, 768)\n",
      "914 (914, 768)\n",
      "922 (922, 768)\n",
      "327 (327, 768)\n",
      "524 (524, 768)\n",
      "925 (925, 768)\n",
      "247 (247, 768)\n",
      "147 (147, 768)\n",
      "283 (283, 768)\n",
      "213 (213, 768)\n",
      "215 (215, 768)\n",
      "307 (307, 768)\n",
      "635 (635, 768)\n",
      "815 (815, 768)\n",
      "582 (582, 768)\n",
      "522 (522, 768)\n",
      "587 (587, 768)\n",
      "683 (683, 768)\n",
      "323 (323, 768)\n",
      "279 (279, 768)\n",
      "377 (377, 768)\n",
      "380 (380, 768)\n",
      "506 (506, 768)\n",
      "87 (87, 768)\n",
      "251 (251, 768)\n",
      "331 (331, 768)\n",
      "656 (656, 768)\n",
      "616 (616, 768)\n",
      "647 (647, 768)\n",
      "437 (437, 768)\n",
      "431 (431, 768)\n",
      "457 (457, 768)\n",
      "453 (453, 768)\n",
      "197 (197, 768)\n",
      "315 (315, 768)\n",
      "450 (450, 768)\n",
      "122 (122, 768)\n",
      "222 (222, 768)\n",
      "262 (262, 768)\n",
      "141 (141, 768)\n",
      "1204 (1204, 768)\n",
      "1015 (1015, 768)\n",
      "572 (572, 768)\n",
      "585 (585, 768)\n",
      "366 (366, 768)\n",
      "397 (397, 768)\n",
      "804 (804, 768)\n",
      "1302 (1302, 768)\n",
      "950 (950, 768)\n",
      "1288 (1288, 768)\n",
      "927 (927, 768)\n",
      "1509 (1509, 768)\n",
      "1373 (1373, 768)\n",
      "657 (657, 768)\n",
      "1470 (1470, 768)\n",
      "833 (833, 768)\n",
      "911 (911, 768)\n",
      "919 (919, 768)\n",
      "778 (778, 768)\n",
      "889 (889, 768)\n",
      "245 (245, 768)\n",
      "536 (536, 768)\n",
      "247 (247, 768)\n",
      "544 (544, 768)\n",
      "363 (363, 768)\n",
      "501 (501, 768)\n",
      "665 (665, 768)\n",
      "1433 (1433, 768)\n",
      "2204 (2204, 768)\n",
      "1571 (1571, 768)\n",
      "1995 (1995, 768)\n",
      "561 (561, 768)\n",
      "832 (832, 768)\n",
      "994 (994, 768)\n",
      "1092 (1092, 768)\n",
      "1055 (1055, 768)\n",
      "790 (790, 768)\n",
      "1376 (1376, 768)\n",
      "427 (427, 768)\n",
      "401 (401, 768)\n",
      "493 (493, 768)\n",
      "311 (311, 768)\n",
      "261 (261, 768)\n",
      "372 (372, 768)\n",
      "695 (695, 768)\n",
      "1128 (1128, 768)\n",
      "909 (909, 768)\n",
      "939 (939, 768)\n",
      "973 (973, 768)\n",
      "974 (974, 768)\n",
      "944 (944, 768)\n",
      "836 (836, 768)\n",
      "862 (862, 768)\n",
      "768 (768, 768)\n",
      "446 (446, 768)\n",
      "641 (641, 768)\n",
      "218 (218, 768)\n",
      "394 (394, 768)\n",
      "717 (717, 768)\n",
      "645 (645, 768)\n",
      "723 (723, 768)\n",
      "876 (876, 768)\n",
      "721 (721, 768)\n",
      "845 (845, 768)\n",
      "844 (844, 768)\n",
      "463 (463, 768)\n",
      "465 (465, 768)\n",
      "291 (291, 768)\n",
      "658 (658, 768)\n",
      "362 (362, 768)\n",
      "477 (477, 768)\n",
      "579 (579, 768)\n",
      "654 (654, 768)\n",
      "280 (280, 768)\n",
      "854 (854, 768)\n",
      "725 (725, 768)\n",
      "1233 (1233, 768)\n",
      "347 (347, 768)\n",
      "981 (981, 768)\n",
      "1065 (1065, 768)\n",
      "373 (373, 768)\n",
      "1578 (1578, 768)\n",
      "1841 (1841, 768)\n",
      "1250 (1250, 768)\n",
      "412 (412, 768)\n",
      "1276 (1276, 768)\n",
      "981 (981, 768)\n",
      "1140 (1140, 768)\n",
      "846 (846, 768)\n",
      "1197 (1197, 768)\n",
      "1551 (1551, 768)\n",
      "1350 (1350, 768)\n",
      "572 (572, 768)\n",
      "282 (282, 768)\n",
      "175 (175, 768)\n",
      "395 (395, 768)\n",
      "202 (202, 768)\n",
      "672 (672, 768)\n",
      "723 (723, 768)\n",
      "181 (181, 768)\n",
      "404 (404, 768)\n",
      "312 (312, 768)\n",
      "243 (243, 768)\n",
      "142 (142, 768)\n",
      "175 (175, 768)\n",
      "211 (211, 768)\n",
      "690 (690, 768)\n",
      "636 (636, 768)\n",
      "553 (553, 768)\n",
      "792 (792, 768)\n",
      "967 (967, 768)\n",
      "867 (867, 768)\n",
      "1564 (1564, 768)\n",
      "1006 (1006, 768)\n",
      "1618 (1618, 768)\n",
      "1448 (1448, 768)\n",
      "1384 (1384, 768)\n",
      "377 (377, 768)\n",
      "2143 (2143, 768)\n",
      "2159 (2159, 768)\n",
      "667 (667, 768)\n",
      "1035 (1035, 768)\n",
      "1000 (1000, 768)\n",
      "482 (482, 768)\n",
      "436 (436, 768)\n",
      "521 (521, 768)\n",
      "576 (576, 768)\n",
      "551 (551, 768)\n",
      "592 (592, 768)\n",
      "294 (294, 768)\n",
      "483 (483, 768)\n",
      "630 (630, 768)\n",
      "790 (790, 768)\n",
      "1115 (1115, 768)\n",
      "1113 (1113, 768)\n",
      "1167 (1167, 768)\n",
      "664 (664, 768)\n",
      "753 (753, 768)\n",
      "945 (945, 768)\n",
      "303 (303, 768)\n",
      "1078 (1078, 768)\n",
      "841 (841, 768)\n",
      "455 (455, 768)\n",
      "450 (450, 768)\n",
      "764 (764, 768)\n",
      "1694 (1694, 768)\n",
      "1558 (1558, 768)\n",
      "699 (699, 768)\n",
      "1306 (1306, 768)\n",
      "235 (235, 768)\n",
      "622 (622, 768)\n",
      "379 (379, 768)\n",
      "416 (416, 768)\n",
      "762 (762, 768)\n",
      "436 (436, 768)\n",
      "402 (402, 768)\n",
      "900 (900, 768)\n",
      "605 (605, 768)\n",
      "492 (492, 768)\n",
      "603 (603, 768)\n",
      "623 (623, 768)\n",
      "271 (271, 768)\n",
      "1064 (1064, 768)\n",
      "839 (839, 768)\n",
      "357 (357, 768)\n",
      "400 (400, 768)\n",
      "579 (579, 768)\n",
      "663 (663, 768)\n",
      "1083 (1083, 768)\n",
      "1306 (1306, 768)\n",
      "764 (764, 768)\n",
      "509 (509, 768)\n",
      "595 (595, 768)\n",
      "720 (720, 768)\n",
      "896 (896, 768)\n",
      "1225 (1225, 768)\n",
      "1001 (1001, 768)\n",
      "793 (793, 768)\n",
      "903 (903, 768)\n",
      "349 (349, 768)\n",
      "539 (539, 768)\n",
      "712 (712, 768)\n",
      "111 (111, 768)\n",
      "177 (177, 768)\n",
      "1843 (1843, 768)\n",
      "1618 (1618, 768)\n",
      "1351 (1351, 768)\n",
      "98 (98, 768)\n",
      "110 (110, 768)\n",
      "146 (146, 768)\n",
      "461 (461, 768)\n",
      "394 (394, 768)\n",
      "834 (834, 768)\n",
      "938 (938, 768)\n",
      "1097 (1097, 768)\n",
      "1207 (1207, 768)\n",
      "602 (602, 768)\n",
      "842 (842, 768)\n",
      "646 (646, 768)\n",
      "620 (620, 768)\n",
      "1243 (1243, 768)\n",
      "1208 (1208, 768)\n",
      "395 (395, 768)\n",
      "400 (400, 768)\n",
      "332 (332, 768)\n",
      "457 (457, 768)\n",
      "252 (252, 768)\n",
      "253 (253, 768)\n",
      "209 (209, 768)\n",
      "169 (169, 768)\n",
      "211 (211, 768)\n",
      "243 (243, 768)\n",
      "688 (688, 768)\n",
      "856 (856, 768)\n",
      "1251 (1251, 768)\n",
      "1041 (1041, 768)\n",
      "1491 (1491, 768)\n",
      "1533 (1533, 768)\n",
      "837 (837, 768)\n",
      "804 (804, 768)\n",
      "1031 (1031, 768)\n",
      "451 (451, 768)\n",
      "371 (371, 768)\n",
      "645 (645, 768)\n",
      "176 (176, 768)\n",
      "712 (712, 768)\n",
      "523 (523, 768)\n",
      "293 (293, 768)\n",
      "439 (439, 768)\n",
      "334 (334, 768)\n",
      "375 (375, 768)\n",
      "276 (276, 768)\n",
      "391 (391, 768)\n",
      "1318 (1318, 768)\n",
      "1652 (1652, 768)\n",
      "1692 (1692, 768)\n",
      "1433 (1433, 768)\n",
      "1146 (1146, 768)\n",
      "1866 (1866, 768)\n",
      "1767 (1767, 768)\n",
      "606 (606, 768)\n",
      "953 (953, 768)\n",
      "840 (840, 768)\n",
      "1200 (1200, 768)\n",
      "832 (832, 768)\n",
      "694 (694, 768)\n",
      "1857 (1857, 768)\n",
      "1599 (1599, 768)\n",
      "900 (900, 768)\n",
      "2008 (2008, 768)\n",
      "833 (833, 768)\n",
      "418 (418, 768)\n",
      "866 (866, 768)\n",
      "560 (560, 768)\n",
      "653 (653, 768)\n",
      "756 (756, 768)\n",
      "452 (452, 768)\n",
      "769 (769, 768)\n",
      "800 (800, 768)\n",
      "464 (464, 768)\n",
      "500 (500, 768)\n",
      "275 (275, 768)\n",
      "275 (275, 768)\n",
      "286 (286, 768)\n",
      "282 (282, 768)\n",
      "324 (324, 768)\n",
      "323 (323, 768)\n",
      "259 (259, 768)\n",
      "301 (301, 768)\n",
      "343 (343, 768)\n",
      "208 (208, 768)\n",
      "293 (293, 768)\n",
      "427 (427, 768)\n",
      "1066 (1066, 768)\n",
      "424 (424, 768)\n",
      "338 (338, 768)\n",
      "410 (410, 768)\n",
      "398 (398, 768)\n",
      "520 (520, 768)\n",
      "428 (428, 768)\n",
      "862 (862, 768)\n",
      "787 (787, 768)\n",
      "676 (676, 768)\n",
      "565 (565, 768)\n",
      "478 (478, 768)\n",
      "758 (758, 768)\n",
      "431 (431, 768)\n",
      "450 (450, 768)\n",
      "777 (777, 768)\n",
      "678 (678, 768)\n",
      "479 (479, 768)\n",
      "762 (762, 768)\n",
      "275 (275, 768)\n",
      "289 (289, 768)\n",
      "242 (242, 768)\n",
      "245 (245, 768)\n",
      "246 (246, 768)\n",
      "171 (171, 768)\n",
      "297 (297, 768)\n",
      "409 (409, 768)\n",
      "258 (258, 768)\n",
      "387 (387, 768)\n",
      "391 (391, 768)\n",
      "301 (301, 768)\n",
      "336 (336, 768)\n",
      "401 (401, 768)\n",
      "210 (210, 768)\n",
      "587 (587, 768)\n",
      "424 (424, 768)\n",
      "394 (394, 768)\n",
      "169 (169, 768)\n",
      "390 (390, 768)\n",
      "148 (148, 768)\n",
      "562 (562, 768)\n",
      "860 (860, 768)\n",
      "580 (580, 768)\n",
      "431 (431, 768)\n",
      "281 (281, 768)\n",
      "372 (372, 768)\n",
      "732 (732, 768)\n",
      "363 (363, 768)\n",
      "476 (476, 768)\n",
      "486 (486, 768)\n",
      "505 (505, 768)\n",
      "366 (366, 768)\n",
      "239 (239, 768)\n",
      "300 (300, 768)\n",
      "321 (321, 768)\n",
      "1313 (1313, 768)\n",
      "1011 (1011, 768)\n",
      "1178 (1178, 768)\n",
      "1649 (1649, 768)\n",
      "612 (612, 768)\n",
      "498 (498, 768)\n",
      "456 (456, 768)\n",
      "445 (445, 768)\n",
      "447 (447, 768)\n",
      "668 (668, 768)\n",
      "1647 (1647, 768)\n",
      "784 (784, 768)\n",
      "1032 (1032, 768)\n",
      "928 (928, 768)\n"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "videoname2text_features_sampled={}\n",
    "for videoname in videoname2img_names:\n",
    "    text_embs=text_embeddings[videoname]\n",
    "    x = np.linspace(0, 1, text_embs.shape[0])\n",
    "    f = interp1d(x, text_embs,axis=0)\n",
    "    new_x=np.linspace(0, 1, len(videoname2img_names[videoname]))\n",
    "    videoname2text_features_sampled[videoname]=f(new_x)\n",
    "    print(len(videoname2img_names[videoname]),videoname2text_features_sampled[videoname].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e72e83c3a1b4ad4895e28acfe9602fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_embeddings_list=[]\n",
    "sentences=[]\n",
    "for videoname in tqdm(videoname2text_test):\n",
    "    sentences.append(videoname2text_test[videoname])\n",
    "    if len(sentences)>32:\n",
    "        outputs=classifier(sentences, return_tensors=\"pt\")\n",
    "        for output in outputs:\n",
    "            text_embeddings_list.append(output[0].numpy())\n",
    "        sentences=[]\n",
    "        #break\n",
    "if len(sentences)>0:\n",
    "    outputs=classifier(sentences, return_tensors=\"pt\")\n",
    "    for output in outputs:\n",
    "        text_embeddings_list.append(output[0].numpy())\n",
    "    sentences=[]\n",
    "    \n",
    "text_embeddings_test={}\n",
    "for i,videoname in enumerate(videoname2text_test):\n",
    "    text_embeddings_test[videoname]=text_embeddings_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n"
     ]
    }
   ],
   "source": [
    "AUDIO2TEXT_EMBEDDINGS_TEST='test_bah_text_roberta-base-go_emotions.pickle'\n",
    "#AUDIO2TEXT_EMBEDDINGS='test_bah_text_openai_small.pickle'\n",
    "#AUDIO2TEXT_EMBEDDINGS='test_bah_text_gigachat.pickle'\n",
    "\n",
    "if False:\n",
    "    with open(AUDIO2TEXT_EMBEDDINGS_TEST, 'wb') as handle:\n",
    "        pickle.dump(text_embeddings_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(AUDIO2TEXT_EMBEDDINGS_TEST, 'rb') as handle:\n",
    "        text_embeddings_test=pickle.load(handle)\n",
    "print(len(text_embeddings_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 (418, 768)\n",
      "421 (421, 768)\n",
      "348 (348, 768)\n",
      "368 (368, 768)\n",
      "248 (248, 768)\n",
      "475 (475, 768)\n",
      "349 (349, 768)\n",
      "938 (938, 768)\n",
      "1220 (1220, 768)\n",
      "1238 (1238, 768)\n",
      "1053 (1053, 768)\n",
      "884 (884, 768)\n",
      "1765 (1765, 768)\n",
      "1064 (1064, 768)\n",
      "855 (855, 768)\n",
      "502 (502, 768)\n",
      "488 (488, 768)\n",
      "630 (630, 768)\n",
      "429 (429, 768)\n",
      "558 (558, 768)\n",
      "420 (420, 768)\n",
      "467 (467, 768)\n",
      "985 (985, 768)\n",
      "1119 (1119, 768)\n",
      "827 (827, 768)\n",
      "1267 (1267, 768)\n",
      "635 (635, 768)\n",
      "1118 (1118, 768)\n",
      "439 (439, 768)\n",
      "873 (873, 768)\n",
      "816 (816, 768)\n",
      "1079 (1079, 768)\n",
      "361 (361, 768)\n",
      "420 (420, 768)\n",
      "600 (600, 768)\n",
      "451 (451, 768)\n",
      "341 (341, 768)\n",
      "255 (255, 768)\n",
      "384 (384, 768)\n",
      "285 (285, 768)\n",
      "181 (181, 768)\n",
      "270 (270, 768)\n",
      "371 (371, 768)\n",
      "320 (320, 768)\n",
      "396 (396, 768)\n",
      "421 (421, 768)\n",
      "298 (298, 768)\n",
      "438 (438, 768)\n",
      "409 (409, 768)\n",
      "249 (249, 768)\n",
      "386 (386, 768)\n",
      "207 (207, 768)\n",
      "248 (248, 768)\n",
      "158 (158, 768)\n",
      "254 (254, 768)\n",
      "346 (346, 768)\n",
      "411 (411, 768)\n",
      "569 (569, 768)\n",
      "498 (498, 768)\n",
      "440 (440, 768)\n",
      "394 (394, 768)\n",
      "672 (672, 768)\n",
      "354 (354, 768)\n",
      "505 (505, 768)\n",
      "165 (165, 768)\n",
      "205 (205, 768)\n",
      "410 (410, 768)\n",
      "344 (344, 768)\n",
      "242 (242, 768)\n",
      "252 (252, 768)\n",
      "162 (162, 768)\n",
      "204 (204, 768)\n",
      "130 (130, 768)\n",
      "177 (177, 768)\n",
      "173 (173, 768)\n",
      "144 (144, 768)\n",
      "160 (160, 768)\n",
      "391 (391, 768)\n",
      "500 (500, 768)\n",
      "339 (339, 768)\n",
      "298 (298, 768)\n",
      "323 (323, 768)\n",
      "340 (340, 768)\n",
      "340 (340, 768)\n",
      "676 (676, 768)\n",
      "552 (552, 768)\n",
      "982 (982, 768)\n",
      "1044 (1044, 768)\n",
      "817 (817, 768)\n",
      "1087 (1087, 768)\n",
      "962 (962, 768)\n",
      "459 (459, 768)\n",
      "263 (263, 768)\n",
      "299 (299, 768)\n",
      "418 (418, 768)\n",
      "234 (234, 768)\n",
      "205 (205, 768)\n",
      "185 (185, 768)\n",
      "696 (696, 768)\n",
      "382 (382, 768)\n",
      "675 (675, 768)\n",
      "454 (454, 768)\n",
      "322 (322, 768)\n",
      "387 (387, 768)\n",
      "385 (385, 768)\n",
      "307 (307, 768)\n",
      "586 (586, 768)\n",
      "608 (608, 768)\n",
      "406 (406, 768)\n",
      "388 (388, 768)\n",
      "481 (481, 768)\n",
      "752 (752, 768)\n",
      "577 (577, 768)\n",
      "567 (567, 768)\n",
      "411 (411, 768)\n",
      "445 (445, 768)\n",
      "657 (657, 768)\n",
      "401 (401, 768)\n",
      "713 (713, 768)\n",
      "1136 (1136, 768)\n",
      "1460 (1460, 768)\n",
      "1350 (1350, 768)\n",
      "1375 (1375, 768)\n",
      "1266 (1266, 768)\n",
      "1731 (1731, 768)\n",
      "1718 (1718, 768)\n",
      "200 (200, 768)\n",
      "146 (146, 768)\n",
      "274 (274, 768)\n",
      "202 (202, 768)\n",
      "313 (313, 768)\n",
      "252 (252, 768)\n",
      "298 (298, 768)\n",
      "397 (397, 768)\n",
      "477 (477, 768)\n",
      "433 (433, 768)\n",
      "459 (459, 768)\n",
      "340 (340, 768)\n",
      "343 (343, 768)\n",
      "516 (516, 768)\n",
      "266 (266, 768)\n",
      "397 (397, 768)\n",
      "350 (350, 768)\n",
      "761 (761, 768)\n",
      "271 (271, 768)\n",
      "472 (472, 768)\n",
      "595 (595, 768)\n",
      "863 (863, 768)\n",
      "1273 (1273, 768)\n",
      "1576 (1576, 768)\n",
      "768 (768, 768)\n",
      "954 (954, 768)\n",
      "693 (693, 768)\n",
      "814 (814, 768)\n",
      "667 (667, 768)\n",
      "920 (920, 768)\n",
      "980 (980, 768)\n",
      "950 (950, 768)\n",
      "940 (940, 768)\n",
      "981 (981, 768)\n",
      "545 (545, 768)\n",
      "207 (207, 768)\n",
      "264 (264, 768)\n",
      "334 (334, 768)\n",
      "273 (273, 768)\n",
      "232 (232, 768)\n",
      "121 (121, 768)\n",
      "328 (328, 768)\n",
      "512 (512, 768)\n",
      "358 (358, 768)\n",
      "561 (561, 768)\n",
      "283 (283, 768)\n",
      "411 (411, 768)\n",
      "261 (261, 768)\n",
      "299 (299, 768)\n",
      "436 (436, 768)\n",
      "434 (434, 768)\n",
      "374 (374, 768)\n",
      "274 (274, 768)\n",
      "581 (581, 768)\n",
      "546 (546, 768)\n",
      "355 (355, 768)\n",
      "742 (742, 768)\n",
      "1272 (1272, 768)\n",
      "1030 (1030, 768)\n",
      "1136 (1136, 768)\n",
      "1128 (1128, 768)\n",
      "719 (719, 768)\n",
      "1437 (1437, 768)\n",
      "703 (703, 768)\n",
      "1599 (1599, 768)\n",
      "1286 (1286, 768)\n",
      "1188 (1188, 768)\n",
      "801 (801, 768)\n",
      "1114 (1114, 768)\n",
      "1376 (1376, 768)\n",
      "556 (556, 768)\n",
      "735 (735, 768)\n",
      "865 (865, 768)\n",
      "706 (706, 768)\n",
      "623 (623, 768)\n",
      "533 (533, 768)\n",
      "583 (583, 768)\n",
      "308 (308, 768)\n",
      "313 (313, 768)\n",
      "644 (644, 768)\n",
      "323 (323, 768)\n",
      "326 (326, 768)\n",
      "334 (334, 768)\n",
      "427 (427, 768)\n",
      "227 (227, 768)\n",
      "551 (551, 768)\n",
      "609 (609, 768)\n",
      "550 (550, 768)\n",
      "343 (343, 768)\n",
      "682 (682, 768)\n",
      "641 (641, 768)\n",
      "434 (434, 768)\n",
      "340 (340, 768)\n",
      "280 (280, 768)\n",
      "533 (533, 768)\n",
      "491 (491, 768)\n",
      "412 (412, 768)\n",
      "312 (312, 768)\n",
      "461 (461, 768)\n",
      "671 (671, 768)\n",
      "1066 (1066, 768)\n",
      "480 (480, 768)\n",
      "384 (384, 768)\n",
      "825 (825, 768)\n",
      "615 (615, 768)\n",
      "143 (143, 768)\n",
      "168 (168, 768)\n",
      "184 (184, 768)\n",
      "199 (199, 768)\n",
      "146 (146, 768)\n",
      "188 (188, 768)\n",
      "182 (182, 768)\n",
      "667 (667, 768)\n",
      "949 (949, 768)\n",
      "836 (836, 768)\n",
      "1049 (1049, 768)\n",
      "1087 (1087, 768)\n",
      "1266 (1266, 768)\n",
      "682 (682, 768)\n",
      "451 (451, 768)\n",
      "446 (446, 768)\n",
      "563 (563, 768)\n",
      "280 (280, 768)\n",
      "630 (630, 768)\n",
      "877 (877, 768)\n",
      "820 (820, 768)\n",
      "255 (255, 768)\n",
      "525 (525, 768)\n",
      "489 (489, 768)\n",
      "367 (367, 768)\n",
      "314 (314, 768)\n",
      "228 (228, 768)\n",
      "291 (291, 768)\n",
      "434 (434, 768)\n",
      "898 (898, 768)\n",
      "1235 (1235, 768)\n",
      "1561 (1561, 768)\n",
      "664 (664, 768)\n",
      "945 (945, 768)\n",
      "739 (739, 768)\n",
      "801 (801, 768)\n",
      "981 (981, 768)\n",
      "1166 (1166, 768)\n",
      "1264 (1264, 768)\n",
      "785 (785, 768)\n",
      "1238 (1238, 768)\n",
      "1072 (1072, 768)\n",
      "643 (643, 768)\n",
      "1763 (1763, 768)\n",
      "2295 (2295, 768)\n",
      "1144 (1144, 768)\n",
      "1750 (1750, 768)\n",
      "560 (560, 768)\n",
      "1109 (1109, 768)\n"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "videoname2text_features_sampled_test={}\n",
    "for videoname in videoname2img_names_test:\n",
    "    text_embs=text_embeddings_test[videoname]\n",
    "    x = np.linspace(0, 1, text_embs.shape[0])\n",
    "    f = interp1d(x, text_embs,axis=0)\n",
    "    new_x=np.linspace(0, 1, len(videoname2img_names_test[videoname]))\n",
    "    videoname2text_features_sampled_test[videoname]=f(new_x)\n",
    "    print(len(videoname2img_names_test[videoname]),videoname2text_features_sampled_test[videoname].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ANNOTATION_FRAME_LABELS, 'r') as f:\n",
    "    content = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['fr_detailed_ah', 'frame_annotation', 'global_ah', 'time_detailed_ah']),\n",
       " 431)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['82557/Visite 1/82557_Question_1_2024-08-22 14-46-11_Video.mp4'].keys(),len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "def get_videonames(filename):\n",
    "    videoname2AH={}\n",
    "    with open(os.path.join(DATA_DIR,filename)) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            splitted_line=line.split(',')\n",
    "            videoname=splitted_line[0]\n",
    "            target=int(splitted_line[1])\n",
    "            videoname2AH[videoname]=target\n",
    "    print(len(videoname2AH))\n",
    "    return videoname2AH\n",
    "                \n",
    "videoname2AH_train=get_videonames(TRAIN_LABELS)\n",
    "videoname2AH_val=get_videonames(VAL_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89600, 768) (89600,)\n",
      "(205926, 768) (205926,)\n"
     ]
    }
   ],
   "source": [
    "def get_video2targets(videoname2features,videoname2AH,step=1):\n",
    "    X,y=[],[]\n",
    "    for videoname in videoname2AH:\n",
    "        frame2ind={}\n",
    "        for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "            frame2ind[frame]=ind\n",
    "        features=videoname2features[videoname]\n",
    "        for frame,AH in content[videoname]['frame_annotation']:\n",
    "            ind=frame2ind[frame]\n",
    "            if AH==1 or ind%step==0:\n",
    "                X.append(features[ind])\n",
    "                y.append(AH)\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    print(X.shape,y.shape)\n",
    "    return X,y\n",
    "\n",
    "if False:\n",
    "    X_val,y_val=get_video2targets(videoname2audio_features_sampled,videoname2AH_val,step=1)\n",
    "    X_train,y_train=get_video2targets(videoname2audio_features_sampled,videoname2AH_train,step=1)\n",
    "elif True:\n",
    "    X_val,y_val=get_video2targets(videoname2text_features_sampled,videoname2AH_val,step=1)\n",
    "    X_train,y_train=get_video2targets(videoname2text_features_sampled,videoname2AH_train,step=1)\n",
    "elif False:\n",
    "    X_val,y_val=get_video2targets(videoname2features,videoname2AH_val,step=1)\n",
    "    X_train,y_train=get_video2targets(videoname2features,videoname2AH_train,step=1) #4)\n",
    "else:\n",
    "    X_val,y_val=get_video2targets(videoname2scores,videoname2AH_val,step=1)\n",
    "    X_train,y_train=get_video2targets(videoname2scores,videoname2AH_train,step=1) #4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89600, 1536) (89600,)\n",
      "(80730, 1536) (80730,)\n"
     ]
    }
   ],
   "source": [
    "def get_text2targets(videoname2AH,step=1):\n",
    "    X,y=[],[]\n",
    "    for videoname in videoname2AH:\n",
    "        frame2ind={}\n",
    "        for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "            frame2ind[frame]=ind\n",
    "        text_embs=text_embeddings[videoname]\n",
    "        for frame,AH in content[videoname]['frame_annotation']:\n",
    "            if AH==1 or ind%step==0:\n",
    "                X.append(text_embs)\n",
    "                y.append(AH)\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    print(X.shape,y.shape)\n",
    "    return X,y\n",
    "\n",
    "X_val,y_val=get_text2targets(videoname2AH_val,step=1)\n",
    "X_train,y_train=get_text2targets(videoname2AH_train,step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89600, 1802) (89600,)\n",
      "(205926, 1802) (205926,)\n"
     ]
    }
   ],
   "source": [
    "def get_concat_features2targets(videoname2AH,step=1):\n",
    "    X,y=[],[]\n",
    "    for videoname in videoname2AH:\n",
    "        frame2ind={}\n",
    "        for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "            frame2ind[frame]=ind\n",
    "        video_features=videoname2scores[videoname]\n",
    "        #text_embs=text_embeddings[videoname]\n",
    "        text_embs=text_embeddings[videoname].mean(axis=0)\n",
    "        audio_features=videoname2audio_features_sampled[videoname]\n",
    "        text_features=videoname2text_features_sampled[videoname]\n",
    "        for frame,AH in content[videoname]['frame_annotation']:\n",
    "            ind=frame2ind[frame]\n",
    "            if AH==1 or ind%step==0:\n",
    "                #X.append(np.concatenate((video_features[ind],text_embs),axis=0))\n",
    "                \n",
    "                #X.append(np.concatenate((video_features[ind],text_features[ind]),axis=0))\n",
    "                #X.append(video_features[ind])\n",
    "                #X.append(audio_features[ind])\n",
    "                #X.append(np.concatenate((video_features[ind],audio_features[ind]),axis=0))\n",
    "                X.append(np.concatenate((video_features[ind],audio_features[ind],text_embs),axis=0))\n",
    "                #X.append(np.concatenate((audio_features[ind],text_embs),axis=0))\n",
    "                #X.append(np.concatenate((video_features[ind],audio_features[ind],text_features[ind]),axis=0))\n",
    "                \n",
    "                y.append(AH)\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    print(X.shape,y.shape)\n",
    "    return X,y\n",
    "\n",
    "X_val,y_val=get_concat_features2targets(videoname2AH_val,step=1)\n",
    "X_train,y_train=get_concat_features2targets(videoname2AH_train,step=1) #4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295526, 768) (295526,)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    TRAIN_VAL=True\n",
    "    X_train=np.concatenate((X_train,X_val))\n",
    "    y_train=np.concatenate((y_train,y_val))\n",
    "    print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '(val)')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 216x216 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "axs[0].hist(y_train, bins=20)\n",
    "axs[0].set_title('(Train)')\n",
    "axs[1].hist(y_val, bins=20)\n",
    "axs[1].set_title('(val)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89600,)\n",
      "0.674065070583054\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "res=RandomForestClassifier(n_estimators=1000,max_depth=3, n_jobs=-1)\n",
    "#res=svm.LinearSVC(C=1)\n",
    "#import xgboost as xgb\n",
    "#res=xgb.XGBRegressor(n_estimators=10)\n",
    "#clf = xgb.XGBClassifier(n_estimators=1000,use_label_encoder=False)\n",
    "\n",
    "res.fit(X_train, y_train)\n",
    "y_val_preds = res.predict(X_val)\n",
    "print(y_val_preds.shape)\n",
    "print(f1_score(y_val,y_val_preds,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89600, 2)\n",
      "0.0 0.08397536141640863\n",
      "0.1 0.23923699613906446\n",
      "0.2 0.615172904729874\n",
      "0.30000000000000004 0.7061321512010741\n",
      "0.4 0.674065070583054\n",
      "0.5 0.674065070583054\n",
      "0.6000000000000001 0.674065070583054\n",
      "0.7000000000000001 0.674065070583054\n",
      "0.8 0.674065070583054\n",
      "0.9 0.674065070583054\n"
     ]
    }
   ],
   "source": [
    "y_val_preds = res.predict_proba(X_val)\n",
    "print(y_val_preds.shape)\n",
    "for t in np.linspace(0,0.9,10):\n",
    "    new_pred = (y_val_preds[:,1] >= t)\n",
    "    print(t,f1_score(y_val,new_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model,Sequential, load_model,model_from_json\n",
    "from tensorflow.keras.applications import mobilenet,mobilenet_v2,densenet,inception_resnet_v2,inception_v3,vgg16,resnet_v2,resnet\n",
    "import efficientnet.tfkeras as enet\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout,GlobalAveragePooling2D,Activation, Conv2D, Reshape,DepthwiseConv2D,Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, CSVLogger, EarlyStopping\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow.keras.backend as K \n",
    "\n",
    "print(tf.__version__)\n",
    "from tensorflow.compat.v1.keras.backend import set_session \n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.compat.v1.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class SaveBestModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_best_metric='val_loss', this_max=False):\n",
    "        self.save_best_metric = save_best_metric\n",
    "        self.max = this_max\n",
    "        if this_max:\n",
    "            self.best = float('-inf')\n",
    "        else:\n",
    "            self.best = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metric_value = logs[self.save_best_metric]\n",
    "        if self.max:\n",
    "            if metric_value > self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_model_weights = deepcopy(self.model.get_weights())\n",
    "\n",
    "        else:\n",
    "            if metric_value < self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_model_weights = deepcopy(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165140 40786 4.0489383612023735\n",
      "{0: 0.623489160712123, 1: 2.5244691806011867} [0.62348916 2.52446918]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0, 1], y=[1 1 1 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "neg, pos = np.bincount(y_train)\n",
    "print(neg, pos, neg/pos)\n",
    "total = neg + pos\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weights={0:weight_for_0,1:weight_for_1}\n",
    "#class_weights[i] = compute_class_weight('balanced', [0,1], y_train[:, i])\n",
    "print(class_weights,compute_class_weight('balanced', [0,1], y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics():\n",
    "    y_val_preds=mlpModel.predict(X_val,verbose=0)\n",
    "    new_pred = (y_val_preds >= 0.5)\n",
    "    print(f1_score(y_val,new_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    loss='binary_crossentropy'\n",
    "    #loss='hinge'\n",
    "else:\n",
    "    import tensorflow.keras.backend as K\n",
    "    def get_weighted_loss(weights):\n",
    "        def weighted_loss(y_true, y_pred):\n",
    "            y_true=tf.cast(y_true, tf.float32)\n",
    "            ce=K.binary_crossentropy(y_true, y_pred)\n",
    "            return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*ce, axis=-1)\n",
    "        return weighted_loss\n",
    "    loss=get_weighted_loss(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def relu_advanced(x):\n",
    "    return K.relu(x, max_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision()] # \n",
    "metrics=[tf.keras.metrics.F1Score(average='weighted',threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 64)                49216     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49281 (192.50 KB)\n",
      "Trainable params: 49281 (192.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "578/578 [==============================] - 5s 7ms/step - loss: 0.4017 - f1_score: 0.3245 - val_loss: 0.3777 - val_f1_score: 0.5075\n",
      "0.5075235962867737\n",
      "0.7996352957419591\n",
      "Best weights:\n",
      "0.7996352957419591\n"
     ]
    }
   ],
   "source": [
    "batch_size=512 #256 #128 #1024 #\n",
    "mlpModel=Sequential()\n",
    "mlpModel.add(Dense(64, input_shape=X_train.shape[1:],activation='relu')) #256\n",
    "mlpModel.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "mlpModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), loss='binary_crossentropy', metrics=metrics)\n",
    "mlpModel.summary()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "#save_best_model = SaveBestModel('val_loss',False)\n",
    "save_best_model = SaveBestModel('val_f1_score',True)\n",
    "\n",
    "#num_epochs=20\n",
    "num_epochs=1\n",
    "mlpModel.fit(X_train,y_train.astype(np.float64), batch_size=batch_size, epochs=num_epochs, verbose=1, #class_weight=class_weights, \n",
    "             callbacks=[save_best_model], validation_data=(X_val,y_val.astype(np.float64)))\n",
    "best_model_weights = save_best_model.best_model_weights\n",
    "print(save_best_model.best)\n",
    "\n",
    "print_metrics()\n",
    "if True:\n",
    "    print('Best weights:')\n",
    "    mlpModel.set_weights(best_model_weights)\n",
    "    print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 1)                 769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 769 (3.00 KB)\n",
      "Trainable params: 769 (3.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "403/403 [==============================] - 3s 7ms/step - loss: 0.5802 - f1_score: 0.4509 - val_loss: 0.6719 - val_f1_score: 0.3777\n",
      "Epoch 2/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.5262 - f1_score: 0.5283 - val_loss: 0.6998 - val_f1_score: 0.3861\n",
      "Epoch 3/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.5091 - f1_score: 0.5457 - val_loss: 0.6780 - val_f1_score: 0.3824\n",
      "Epoch 4/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.5000 - f1_score: 0.5559 - val_loss: 0.6902 - val_f1_score: 0.3798\n",
      "Epoch 5/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4944 - f1_score: 0.5617 - val_loss: 0.7083 - val_f1_score: 0.3908\n",
      "Epoch 6/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4911 - f1_score: 0.5668 - val_loss: 0.6950 - val_f1_score: 0.3751\n",
      "Epoch 7/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4885 - f1_score: 0.5704 - val_loss: 0.7359 - val_f1_score: 0.3924\n",
      "Epoch 8/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4869 - f1_score: 0.5731 - val_loss: 0.7258 - val_f1_score: 0.3864\n",
      "Epoch 9/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4857 - f1_score: 0.5760 - val_loss: 0.7184 - val_f1_score: 0.3851\n",
      "Epoch 10/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4850 - f1_score: 0.5766 - val_loss: 0.7296 - val_f1_score: 0.3901\n",
      "Epoch 11/20\n",
      "403/403 [==============================] - 3s 7ms/step - loss: 0.4842 - f1_score: 0.5787 - val_loss: 0.7573 - val_f1_score: 0.3850\n",
      "Epoch 12/20\n",
      "403/403 [==============================] - 3s 7ms/step - loss: 0.4836 - f1_score: 0.5796 - val_loss: 0.7342 - val_f1_score: 0.3846\n",
      "Epoch 13/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4838 - f1_score: 0.5799 - val_loss: 0.7323 - val_f1_score: 0.3858\n",
      "Epoch 14/20\n",
      "403/403 [==============================] - 3s 7ms/step - loss: 0.4833 - f1_score: 0.5810 - val_loss: 0.7453 - val_f1_score: 0.3872\n",
      "Epoch 15/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4828 - f1_score: 0.5820 - val_loss: 0.7851 - val_f1_score: 0.3861\n",
      "Epoch 16/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4828 - f1_score: 0.5819 - val_loss: 0.7162 - val_f1_score: 0.3765\n",
      "Epoch 17/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4827 - f1_score: 0.5829 - val_loss: 0.7456 - val_f1_score: 0.3885\n",
      "Epoch 18/20\n",
      "403/403 [==============================] - 3s 7ms/step - loss: 0.4822 - f1_score: 0.5834 - val_loss: 0.7107 - val_f1_score: 0.3801\n",
      "Epoch 19/20\n",
      "403/403 [==============================] - 3s 7ms/step - loss: 0.4818 - f1_score: 0.5849 - val_loss: 0.7485 - val_f1_score: 0.3874\n",
      "Epoch 20/20\n",
      "403/403 [==============================] - 3s 6ms/step - loss: 0.4820 - f1_score: 0.5851 - val_loss: 0.7320 - val_f1_score: 0.3902\n",
      "0.39240118861198425\n",
      "0.6755134172324513\n"
     ]
    }
   ],
   "source": [
    "batch_size=512 #128 #1024 #\n",
    "mlpModel=Sequential()\n",
    "\n",
    "mlpModel.add(Dense(1, input_shape=X_train.shape[1:],activation='sigmoid',use_bias=True,kernel_regularizer=tf.keras.regularizers.l2(1.0/batch_size)))\n",
    "mlpModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=metrics)\n",
    "mlpModel.summary()\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "#save_best_model = SaveBestModel('val_loss',False)\n",
    "save_best_model = SaveBestModel('val_f1_score',True)\n",
    "mlpModel.fit(X_train,y_train.astype(np.float64), batch_size=batch_size, epochs=20, verbose=1, class_weight=class_weights, \n",
    "             callbacks=[save_best_model], validation_data=(X_val,y_val.astype(np.float64)))\n",
    "best_model_weights = save_best_model.best_model_weights\n",
    "print(save_best_model.best)\n",
    "\n",
    "print_metrics()\n",
    "if False:\n",
    "    print('Best weights:')\n",
    "    mlpModel.set_weights(best_model_weights)\n",
    "    print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.08397536141640863\n",
      "0.1 0.47818043113162284\n",
      "0.2 0.5937707749469368\n",
      "0.30000000000000004 0.6714925215524378\n",
      "0.4 0.7037931223320123\n",
      "0.5 0.714730699552383\n",
      "0.6000000000000001 0.7052262021275646\n",
      "0.7000000000000001 0.6927716785650124\n",
      "0.8 0.6860486898017749\n",
      "0.9 0.6763623541686223\n"
     ]
    }
   ],
   "source": [
    "y_val_preds=mlpModel.predict(X_val,verbose=0)\n",
    "for t in np.linspace(0,0.9,10):\n",
    "    new_pred = (y_val_preds >= t)\n",
    "    print(t,f1_score(y_val,new_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enet_mtl\n",
    "\n",
    "mlp\n",
    "scores\n",
    "0.3 0.683270404865019\n",
    "0.5 0.674065070583054\n",
    "\n",
    "features\n",
    "0.5 0.6845600415252329\n",
    "\n",
    "mbf_va\n",
    "scores\n",
    "0.3 0.69998378811507\n",
    "0.5 0.6745209202464137\n",
    "\n",
    "features\n",
    "0.5 0.6835512457339219\n",
    "\n",
    "1 layer: scores 0.6 0.7058534273480211\n",
    "\n",
    "\n",
    "ddamfnet_mtl\n",
    "scores\n",
    "0.3 0.6876002129077958\n",
    "0.5 0.6740540926333174\n",
    "\n",
    "features\n",
    "0.5 0.6743391260484833\n",
    "0.7 0.6801708851799948\n",
    "\n",
    "mobilevit_va_mtl\n",
    "scores\n",
    "0.3 0.6980500110920965\n",
    "0.5 0.6740916339997399\n",
    "\n",
    "features\n",
    "0.5 0.6769258209353625\n",
    "\n",
    "\n",
    "ddamfnet_8\n",
    "scores\n",
    "0.3 0.6797134586091566\n",
    "0.5 0.6739780341817486\n",
    "\n",
    "features\n",
    "0.4 0.6835845096516041\n",
    "0.5 0.6793618305662475\n",
    "\n",
    "best_vgaf\n",
    "scores\n",
    "0.30000000000000004 0.6974543426153296\n",
    "0.5 0.6744867540403574\n",
    "\n",
    "features\n",
    "0.5 0.664929612944537\n",
    "0.6 0.6765722614182715\n",
    "\n",
    "\n",
    "audio\n",
    "hubert\n",
    "1 hidden 0.5 0.6872779144415531\n",
    "\n",
    "\n",
    "text\n",
    "roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    #mlpModel.save_weights('bah_mbf_va_faces.h5')#0.3 0.7127118641781647 0.5 0.6809112817505317\n",
    "    #mlpModel.save_weights('bah_audio.h5')#1 hidden 0.5 0.6872779144415531\n",
    "    mlpModel.save_weights('bah_text.h5')#1 hidden 0.5 0.714730699552383\n",
    "    #mlpModel.save_weights('bah_mbf_text_32.h5') #0.5 0.7225620550472124\n",
    "else:\n",
    "    mlpModel.load_weights('bah_mbf_va_faces.h5')\n",
    "    #mlpModel.load_weights('bah_mbf_text_32.h5')\n",
    "    #mlpModel.load_weights('bah_audio.h5')\n",
    "    print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    mlpModelFaces=mlpModel\n",
    "    mlpModelFaces.save_weights('bah_mbf_va_faces_train_val.h5')\n",
    "elif False:\n",
    "    mlpModelAudio=mlpModel\n",
    "    mlpModelAudio.save_weights('bah_audio_train_val.h5')\n",
    "else:\n",
    "    mlpModelText=mlpModel\n",
    "    mlpModelText.save_weights('bah_text_train_val.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.08397536141640863\n",
      "0.1 0.12755907473634462\n",
      "0.2 0.5447434879474157\n",
      "0.30000000000000004 0.7127118641781647\n",
      "0.4 0.6953498597573039\n",
      "0.5 0.6809112817505317\n",
      "0.6000000000000001 0.6748963302010191\n",
      "0.7000000000000001 0.6741923717911663\n",
      "0.8 0.674065070583054\n",
      "0.9 0.674065070583054\n",
      "\n",
      "0.3 0.7127118641781647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[61959,  7304],\n",
       "       [15971,  4366]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpModelFaces=mlpModel\n",
    "mlpModelFaces.load_weights('bah_mbf_va_faces.h5')\n",
    "\n",
    "\n",
    "y_val_preds_video=mlpModelFaces.predict(X_val,verbose=0)\n",
    "for t in np.linspace(0,0.9,10):\n",
    "    new_pred = (y_val_preds_video >= t)\n",
    "    print(t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    \n",
    "\n",
    "print()\n",
    "t=0.3\n",
    "new_pred = (y_val_preds_video >= t)\n",
    "print(t,f1_score(y_val,new_pred,average='weighted'))\n",
    "confusion_matrix(y_val,new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.08397536141640863\n",
      "0.1 0.4339121318923602\n",
      "0.2 0.5924394263106827\n",
      "0.30000000000000004 0.6574224910817422\n",
      "0.4 0.6818995536111505\n",
      "0.5 0.6872779144415531\n",
      "0.6000000000000001 0.6843098817662281\n",
      "0.7000000000000001 0.6800358580075511\n",
      "0.8 0.6764023252501593\n",
      "0.9 0.6744735417514716\n",
      "\n",
      "0.5 0.6872779144415531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[63514,  5749],\n",
       "       [18069,  2268]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpModelAudio=mlpModel\n",
    "mlpModelAudio.load_weights('bah_audio.h5')\n",
    "\n",
    "y_val_preds_audio=mlpModelAudio.predict(X_val,verbose=0)\n",
    "for t in np.linspace(0,0.9,10):\n",
    "    new_pred = (y_val_preds_audio >= t)\n",
    "    print(t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    \n",
    "\n",
    "print()\n",
    "\n",
    "t=0.5 #0.3\n",
    "new_pred = (y_val_preds_audio >= t)\n",
    "print(t,f1_score(y_val,new_pred,average='weighted'))\n",
    "\n",
    "\n",
    "confusion_matrix(y_val,new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.08397536141640863\n",
      "0.1 0.47818043113162284\n",
      "0.2 0.5937707749469368\n",
      "0.30000000000000004 0.6714925215524378\n",
      "0.4 0.7037931223320123\n",
      "0.5 0.714730699552383\n",
      "0.6000000000000001 0.7052262021275646\n",
      "0.7000000000000001 0.6927716785650124\n",
      "0.8 0.6860486898017749\n",
      "0.9 0.6763623541686223\n",
      "\n",
      "0.5 0.714730699552383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[61115,  8148],\n",
       "       [15430,  4907]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpModelText=mlpModel\n",
    "mlpModelText.load_weights('bah_text.h5')\n",
    "\n",
    "y_val_preds_text=mlpModelText.predict(X_val,verbose=0)\n",
    "for t in np.linspace(0,0.9,10):\n",
    "    new_pred = (y_val_preds_text >= t)\n",
    "    print(t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    \n",
    "\n",
    "print()\n",
    "t=0.5 #0.7 #0.4 #\n",
    "new_pred = (y_val_preds_text >= t)\n",
    "print(t,f1_score(y_val,new_pred,average='weighted'))\n",
    "\n",
    "confusion_matrix(y_val,new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                704       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 769 (3.00 KB)\n",
      "Trainable params: 769 (3.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65665 (256.50 KB)\n",
      "Trainable params: 65665 (256.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 64)                49216     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49281 (192.50 KB)\n",
      "Trainable params: 49281 (192.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlpModelFaces.summary()\n",
    "mlpModelAudio.summary()\n",
    "mlpModelText.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.2 0.587480577785225\n",
      "0.0 0.3 0.6569651145071818\n",
      "0.0 0.4 0.6824425648151644\n",
      "0.0 0.5 0.6856844252254299\n",
      "0.0 0.6 0.6818757902712668\n",
      "0.0 0.7 0.6773774332689562\n",
      "\n",
      "0.1 0.2 0.5866352081361769\n",
      "0.1 0.3 0.6621055954531865\n",
      "0.1 0.4 0.6846237978878268\n",
      "0.1 0.5 0.6864310157122697\n",
      "0.1 0.6 0.6801884047857105\n",
      "0.1 0.7 0.6762063173837168\n",
      "\n",
      "0.2 0.2 0.5852355501164277\n",
      "0.2 0.3 0.6679271108064185\n",
      "0.2 0.4 0.6869660639051194\n",
      "0.2 0.5 0.6846713211307623\n",
      "0.2 0.6 0.6780094133226198\n",
      "0.2 0.7 0.6748705527918222\n",
      "\n",
      "0.30000000000000004 0.2 0.5815778441052618\n",
      "0.30000000000000004 0.3 0.6749608959012116\n",
      "0.30000000000000004 0.4 0.6888255525260902\n",
      "0.30000000000000004 0.5 0.6828946130495495\n",
      "0.30000000000000004 0.6 0.6765117919574983\n",
      "0.30000000000000004 0.7 0.6741785475584693\n",
      "\n",
      "0.4 0.2 0.5757395805583204\n",
      "0.4 0.3 0.6823738904851496\n",
      "0.4 0.4 0.6902478566072876\n",
      "0.4 0.5 0.6799106753359075\n",
      "0.4 0.6 0.6749649310400466\n",
      "0.4 0.7 0.6740595816427375\n",
      "\n",
      "0.5 0.2 0.5668568115331333\n",
      "0.5 0.3 0.6909951528355646\n",
      "0.5 0.4 0.6898192162893245\n",
      "0.5 0.5 0.6769406742181647\n",
      "0.5 0.6 0.6741950441637707\n",
      "0.5 0.7 0.674065070583054\n",
      "\n",
      "0.6000000000000001 0.2 0.5547114301602671\n",
      "0.6000000000000001 0.3 0.6995517217877075\n",
      "0.6000000000000001 0.4 0.6883729706926226\n",
      "0.6000000000000001 0.5 0.6754979605657095\n",
      "0.6000000000000001 0.6 0.6741923717911663\n",
      "0.6000000000000001 0.7 0.674065070583054\n",
      "\n",
      "0.7000000000000001 0.2 0.5413182907676884\n",
      "0.7000000000000001 0.3 0.7076540418400836\n",
      "0.7000000000000001 0.4 0.6879933853339408\n",
      "0.7000000000000001 0.5 0.6753189515404212\n",
      "0.7000000000000001 0.6 0.6742244189684848\n",
      "0.7000000000000001 0.7 0.6740916339997399\n",
      "\n",
      "0.8 0.2 0.5328881109740221\n",
      "0.8 0.3 0.7132887676106516\n",
      "0.8 0.4 0.6893746035827544\n",
      "0.8 0.5 0.6761829235235117\n",
      "0.8 0.6 0.6742454728689836\n",
      "0.8 0.7 0.6740916339997399\n",
      "\n",
      "0.9 0.2 0.5349640261337127\n",
      "0.9 0.3 0.7151052712154924\n",
      "0.9 0.4 0.6919342531271507\n",
      "0.9 0.5 0.6782857788610089\n",
      "0.9 0.6 0.6744312592720316\n",
      "0.9 0.7 0.6741181952750139\n",
      "\n",
      "1.0 0.2 0.5447434879474157\n",
      "1.0 0.3 0.7127118641781647\n",
      "1.0 0.4 0.6953498597573039\n",
      "1.0 0.5 0.6809112817505317\n",
      "1.0 0.6 0.6748963302010191\n",
      "1.0 0.7 0.6741923717911663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in np.linspace(0,1,11):\n",
    "    y_ensemble=w*y_val_preds_video+(1-w)*y_val_preds_audio\n",
    "    for t in [0.2,0.3,0.4,0.5,0.6,0.7]:\n",
    "        new_pred = (y_ensemble >= t)\n",
    "        print(w,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.2 0.6563537343739165\n",
      "0.0 0.3 0.6766658920570345\n",
      "0.0 0.4 0.6901012544128813\n",
      "0.0 0.5 0.6986103073232967\n",
      "0.0 0.6 0.7049053809039371\n",
      "0.0 0.7 0.707307239042897\n",
      "0.0 0.8 0.7089009531746576\n",
      "\n",
      "0.1 0.2 0.656386512164705\n",
      "0.1 0.3 0.6785344277149241\n",
      "0.1 0.4 0.6924253868720476\n",
      "0.1 0.5 0.701190819615141\n",
      "0.1 0.6 0.7054778574802574\n",
      "0.1 0.7 0.7084795507741691\n",
      "0.1 0.8 0.7079378940105657\n",
      "\n",
      "0.2 0.2 0.6560175454784098\n",
      "0.2 0.3 0.6805119711223927\n",
      "0.2 0.4 0.6949625532838505\n",
      "0.2 0.5 0.7044676141806154\n",
      "0.2 0.6 0.707515080458778\n",
      "0.2 0.7 0.7099264235105053\n",
      "0.2 0.8 0.6999159234952451\n",
      "\n",
      "0.30000000000000004 0.2 0.655397038674081\n",
      "0.30000000000000004 0.3 0.6833367432109794\n",
      "0.30000000000000004 0.4 0.6982534745631517\n",
      "0.30000000000000004 0.5 0.7055984992270606\n",
      "0.30000000000000004 0.6 0.7091273553597373\n",
      "0.30000000000000004 0.7 0.7075686728324977\n",
      "0.30000000000000004 0.8 0.676301757034254\n",
      "\n",
      "0.4 0.2 0.6545800138297471\n",
      "0.4 0.3 0.6870061218281345\n",
      "0.4 0.4 0.7019407593613565\n",
      "0.4 0.5 0.7080901811535892\n",
      "0.4 0.6 0.7105012529096078\n",
      "0.4 0.7 0.6835901395125558\n",
      "0.4 0.8 0.6741813826565727\n",
      "\n",
      "0.5 0.2 0.6500076237337602\n",
      "0.5 0.3 0.6908578495781871\n",
      "0.5 0.4 0.7053197825577052\n",
      "0.5 0.5 0.7106342020903906\n",
      "0.5 0.6 0.6965641136534982\n",
      "0.5 0.7 0.6758008552439166\n",
      "0.5 0.8 0.674065070583054\n",
      "\n",
      "0.6000000000000001 0.2 0.641368558464888\n",
      "0.6000000000000001 0.3 0.6956861986623233\n",
      "0.6000000000000001 0.4 0.710239548515191\n",
      "0.6000000000000001 0.5 0.706912682866515\n",
      "0.6000000000000001 0.6 0.6795818716842873\n",
      "0.6000000000000001 0.7 0.6744778023399355\n",
      "0.6000000000000001 0.8 0.674065070583054\n",
      "\n",
      "0.7000000000000001 0.2 0.6185428780492199\n",
      "0.7000000000000001 0.3 0.7008566786638364\n",
      "0.7000000000000001 0.4 0.714530447191147\n",
      "0.7000000000000001 0.5 0.6878320514831598\n",
      "0.7000000000000001 0.6 0.6758383287431546\n",
      "0.7000000000000001 0.7 0.6741978662556116\n",
      "0.7000000000000001 0.8 0.674065070583054\n",
      "\n",
      "0.8 0.2 0.5839534968284215\n",
      "0.8 0.3 0.7107448215599979\n",
      "0.8 0.4 0.7050288836901468\n",
      "0.8 0.5 0.6805694074186263\n",
      "0.8 0.6 0.6749216556007642\n",
      "0.8 0.7 0.6740916339997399\n",
      "0.8 0.8 0.674065070583054\n",
      "\n",
      "0.9 0.2 0.5503531139701636\n",
      "0.9 0.3 0.7185271779359481\n",
      "0.9 0.4 0.6968104293953882\n",
      "0.9 0.5 0.6794532527755944\n",
      "0.9 0.6 0.6744522860722251\n",
      "0.9 0.7 0.6740916339997399\n",
      "0.9 0.8 0.674065070583054\n",
      "\n",
      "1.0 0.2 0.5447434879474157\n",
      "1.0 0.3 0.7127118641781647\n",
      "1.0 0.4 0.6953498597573039\n",
      "1.0 0.5 0.6809112817505317\n",
      "1.0 0.6 0.6748963302010191\n",
      "1.0 0.7 0.6741923717911663\n",
      "1.0 0.8 0.674065070583054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in np.linspace(0,1,11):\n",
    "    y_ensemble=w*y_val_preds_video+(1-w)*y_val_preds_text\n",
    "    for t in [0.2,0.3,0.4,0.5,0.6, 0.7, 0.8]:\n",
    "        new_pred = (y_ensemble >= t)\n",
    "        print(w,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000000000000001 0.15000000000000002 0.3 0.7203728655057625\n"
     ]
    }
   ],
   "source": [
    "#w1,w2=0.5,0.1\n",
    "best_f1=0\n",
    "best_w1,best_w2,best_t=0,0,0\n",
    "for w1 in np.arange(0.0,0.95,0.05):\n",
    "    for w2 in np.arange(0.0,1-w1,0.05):\n",
    "        y_ensemble=w1*y_val_preds_video+w2*y_val_preds_audio+(1-w1-w2)*y_val_preds_text\n",
    "        for t in [0.2,0.3,0.4,0.5,0.6, 0.7]:\n",
    "            new_pred = (y_ensemble >= t)\n",
    "            f1=f1_score(y_val,new_pred,average='weighted')\n",
    "            #print(w1,w2,t,f1)\n",
    "            if f1>best_f1:\n",
    "                best_f1=f1\n",
    "                best_w1,best_w2,best_t=w1,w2,t\n",
    "        #print()\n",
    "\n",
    "print(best_w1,best_w2,best_t,best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "0.6000000000000001 0.15000000000000002 0.3 0.7203728655057625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.3\n",
    "deltas=[0,5,10,20,30,40,50,60,70,80,100,150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2 0.5447434879474157\n",
      "0 0.3 0.7127118641781647\n",
      "0 0.4 0.6953498597573039\n",
      "0 0.5 0.6809112817505317\n",
      "0 0.6 0.6748963302010191\n",
      "\n",
      "5 0.2 0.5253440435829675\n",
      "5 0.3 0.717759717752158\n",
      "5 0.4 0.6908892184512\n",
      "5 0.5 0.6783720987936441\n",
      "5 0.6 0.674065070583054\n",
      "\n",
      "10 0.2 0.5165326407107852\n",
      "10 0.3 0.7189470865686411\n",
      "10 0.4 0.688465380735041\n",
      "10 0.5 0.6762500197007377\n",
      "10 0.6 0.674065070583054\n",
      "\n",
      "20 0.2 0.5018621513187091\n",
      "20 0.3 0.7201447996128214\n",
      "20 0.4 0.6862859667979138\n",
      "20 0.5 0.6741539086200914\n",
      "20 0.6 0.674065070583054\n",
      "\n",
      "30 0.2 0.49438398998486566\n",
      "30 0.3 0.7169453316869716\n",
      "30 0.4 0.6828421820724406\n",
      "30 0.5 0.674065070583054\n",
      "30 0.6 0.674065070583054\n",
      "\n",
      "40 0.2 0.49110419044798626\n",
      "40 0.3 0.7147305981671971\n",
      "40 0.4 0.6799140794846165\n",
      "40 0.5 0.674065070583054\n",
      "40 0.6 0.674065070583054\n",
      "\n",
      "50 0.2 0.48652714469653263\n",
      "50 0.3 0.7116937973056899\n",
      "50 0.4 0.6791216530923743\n",
      "50 0.5 0.674065070583054\n",
      "50 0.6 0.674065070583054\n",
      "\n",
      "60 0.2 0.4834010739573934\n",
      "60 0.3 0.7088239508318976\n",
      "60 0.4 0.6780097854742909\n",
      "60 0.5 0.674065070583054\n",
      "60 0.6 0.674065070583054\n",
      "\n",
      "70 0.2 0.47910290989833826\n",
      "70 0.3 0.7064131516926917\n",
      "70 0.4 0.6770126639749056\n",
      "70 0.5 0.674065070583054\n",
      "70 0.6 0.674065070583054\n",
      "\n",
      "80 0.2 0.47535354390828005\n",
      "80 0.3 0.7023542560038855\n",
      "80 0.4 0.6768543238216044\n",
      "80 0.5 0.674065070583054\n",
      "80 0.6 0.674065070583054\n",
      "\n",
      "100 0.2 0.4685232428380042\n",
      "100 0.3 0.6990695689415947\n",
      "100 0.4 0.6757343965144307\n",
      "100 0.5 0.674065070583054\n",
      "100 0.6 0.674065070583054\n",
      "\n",
      "150 0.2 0.4580870051488772\n",
      "150 0.3 0.6959583354634623\n",
      "150 0.4 0.6744102280626715\n",
      "150 0.5 0.674065070583054\n",
      "150 0.6 0.674065070583054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#v2f=videoname2features\n",
    "v2f=videoname2scores\n",
    "y_preds_all=[]\n",
    "total_preds=[[] for _ in range(len(deltas))]\n",
    "for videoname in videoname2AH_val:\n",
    "    frame2ind={}\n",
    "    for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "        frame2ind[frame]=ind\n",
    "    features=v2f[videoname]\n",
    "    #text_embs=text_embeddings[videoname]\n",
    "    text_embs=text_embeddings[videoname].mean(axis=0)\n",
    "    X=[]\n",
    "    for frame,AH in content[videoname]['frame_annotation']:\n",
    "        ind=frame2ind[frame]\n",
    "        X.append(features[ind])\n",
    "        #X.append(np.concatenate((features[ind],text_embs),axis=0))\n",
    "        \n",
    "    preds_proba=mlpModelFaces.predict(np.array(X),verbose=0)\n",
    "    for hInd,delta in enumerate(deltas):\n",
    "        for i in range(len(preds_proba)):\n",
    "            i1=max(i-delta,0)\n",
    "            proba=np.mean(preds_proba[i1:i+delta+1])\n",
    "            total_preds[hInd].append(proba)\n",
    "\n",
    "for hInd,delta in enumerate(deltas):\n",
    "    for t in [0.2, 0.3,0.4,0.5,0.6]:\n",
    "        new_pred = (np.array(total_preds[hInd]) >= t)\n",
    "        print(delta,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2 0.5924394263106827\n",
      "0 0.3 0.6574224910817422\n",
      "0 0.4 0.6818995536111505\n",
      "0 0.5 0.6872779144415531\n",
      "0 0.6 0.6843098817662281\n",
      "\n",
      "5 0.2 0.5691313579494216\n",
      "5 0.3 0.6672737733384501\n",
      "5 0.4 0.6912078796576778\n",
      "5 0.5 0.6856026075754004\n",
      "5 0.6 0.6763035613571524\n",
      "\n",
      "10 0.2 0.5640511119883963\n",
      "10 0.3 0.6714111297108663\n",
      "10 0.4 0.6937870509553536\n",
      "10 0.5 0.6830234265316478\n",
      "10 0.6 0.6752082794234838\n",
      "\n",
      "20 0.2 0.5610447271640767\n",
      "20 0.3 0.6752359518357286\n",
      "20 0.4 0.6937485265919809\n",
      "20 0.5 0.6783462010689366\n",
      "20 0.6 0.675221073002222\n",
      "\n",
      "30 0.2 0.5599993859618337\n",
      "30 0.3 0.6778842318816104\n",
      "30 0.4 0.6931812918615786\n",
      "30 0.5 0.6771025903432891\n",
      "30 0.6 0.6753513594905264\n",
      "\n",
      "40 0.2 0.5566574448905988\n",
      "40 0.3 0.6804408505701344\n",
      "40 0.4 0.6916701129129617\n",
      "40 0.5 0.6773816675777979\n",
      "40 0.6 0.674912343687706\n",
      "\n",
      "50 0.2 0.552831701375408\n",
      "50 0.3 0.68231721591337\n",
      "50 0.4 0.6901996564661241\n",
      "50 0.5 0.6773874147574255\n",
      "50 0.6 0.6738303914333398\n",
      "\n",
      "60 0.2 0.5491452155055914\n",
      "60 0.3 0.6815486456969985\n",
      "60 0.4 0.6900532471990092\n",
      "60 0.5 0.6772725701046778\n",
      "60 0.6 0.673297626627592\n",
      "\n",
      "70 0.2 0.5447322784833298\n",
      "70 0.3 0.6803298941287506\n",
      "70 0.4 0.6896458968121129\n",
      "70 0.5 0.6768338007884297\n",
      "70 0.6 0.6728062966482594\n",
      "\n",
      "80 0.2 0.5398382848022755\n",
      "80 0.3 0.6803888788033129\n",
      "80 0.4 0.6881093205461304\n",
      "80 0.5 0.6758595172187231\n",
      "80 0.6 0.6728173060384421\n",
      "\n",
      "100 0.2 0.5327377596468131\n",
      "100 0.3 0.6810138567595928\n",
      "100 0.4 0.6848974409830925\n",
      "100 0.5 0.6754057388099386\n",
      "100 0.6 0.6732409565817815\n",
      "\n",
      "150 0.2 0.5180275963627841\n",
      "150 0.3 0.6832242328963846\n",
      "150 0.4 0.679004877543699\n",
      "150 0.5 0.6727626442473772\n",
      "150 0.6 0.6740376251904217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v2f=videoname2audio_features_sampled\n",
    "y_preds_all=[]\n",
    "total_preds=[[] for _ in range(len(deltas))]\n",
    "for videoname in videoname2AH_val:\n",
    "    frame2ind={}\n",
    "    for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "        frame2ind[frame]=ind\n",
    "    features=v2f[videoname]\n",
    "    X=[]\n",
    "    for frame,AH in content[videoname]['frame_annotation']:\n",
    "        ind=frame2ind[frame]\n",
    "        X.append(features[ind])\n",
    "        \n",
    "    preds_proba=mlpModelAudio.predict(np.array(X),verbose=0)\n",
    "    for hInd,delta in enumerate(deltas):\n",
    "        for i in range(len(preds_proba)):\n",
    "            i1=max(i-delta,0)\n",
    "            proba=np.mean(preds_proba[i1:i+delta+1])\n",
    "            total_preds[hInd].append(proba)\n",
    "\n",
    "for hInd,delta in enumerate(deltas):\n",
    "    for t in [0.2, 0.3,0.4,0.5,0.6]:\n",
    "        new_pred = (np.array(total_preds[hInd]) >= t)\n",
    "        print(delta,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2 0.5937707749469368\n",
      "0 0.3 0.6714925215524378\n",
      "0 0.4 0.7037931223320123\n",
      "0 0.5 0.714730699552383\n",
      "0 0.6 0.7052262021275646\n",
      "0 0.7 0.6927716785650124\n",
      "\n",
      "5 0.2 0.5923886250386629\n",
      "5 0.3 0.6716664484257954\n",
      "5 0.4 0.7045945429352429\n",
      "5 0.5 0.7158123795098356\n",
      "5 0.6 0.7049206687359413\n",
      "5 0.7 0.6929210101875034\n",
      "\n",
      "10 0.2 0.5898066209168007\n",
      "10 0.3 0.6730661099576316\n",
      "10 0.4 0.7051044512943984\n",
      "10 0.5 0.716358171699614\n",
      "10 0.6 0.7038719044171992\n",
      "10 0.7 0.6913243912999057\n",
      "\n",
      "20 0.2 0.5863690177497561\n",
      "20 0.3 0.6731615283326454\n",
      "20 0.4 0.7044727065798922\n",
      "20 0.5 0.7184856715129416\n",
      "20 0.6 0.7049887359254762\n",
      "20 0.7 0.6893615989755374\n",
      "\n",
      "30 0.2 0.5859997964721212\n",
      "30 0.3 0.6723389929087468\n",
      "30 0.4 0.7068690442474534\n",
      "30 0.5 0.7194189646609459\n",
      "30 0.6 0.7020698359626825\n",
      "30 0.7 0.6887736067745202\n",
      "\n",
      "40 0.2 0.5868122013872408\n",
      "40 0.3 0.6710150452817335\n",
      "40 0.4 0.7094767434772875\n",
      "40 0.5 0.7183343754285862\n",
      "40 0.6 0.7002912874532367\n",
      "40 0.7 0.6899763852804338\n",
      "\n",
      "50 0.2 0.585873376269858\n",
      "50 0.3 0.670129583819906\n",
      "50 0.4 0.7105008988008051\n",
      "50 0.5 0.7199169105262578\n",
      "50 0.6 0.7002378919940266\n",
      "50 0.7 0.6900899971436565\n",
      "\n",
      "60 0.2 0.5857842493501595\n",
      "60 0.3 0.6703820363304431\n",
      "60 0.4 0.7101285551829184\n",
      "60 0.5 0.7215147819574829\n",
      "60 0.6 0.7018574337646823\n",
      "60 0.7 0.6919206610854027\n",
      "\n",
      "70 0.2 0.5856543342214696\n",
      "70 0.3 0.6718735574104112\n",
      "70 0.4 0.7089772825460968\n",
      "70 0.5 0.7213858440757142\n",
      "70 0.6 0.701580815971025\n",
      "70 0.7 0.6938289193109464\n",
      "\n",
      "80 0.2 0.5852887208251529\n",
      "80 0.3 0.6730084113655397\n",
      "80 0.4 0.7089020286501841\n",
      "80 0.5 0.7219604631587836\n",
      "80 0.6 0.7019082585779493\n",
      "80 0.7 0.6941277663506282\n",
      "\n",
      "100 0.2 0.583316278943414\n",
      "100 0.3 0.6771898339269246\n",
      "100 0.4 0.7092122050297089\n",
      "100 0.5 0.7248670763995042\n",
      "100 0.6 0.7013247421401624\n",
      "100 0.7 0.6929811278838277\n",
      "\n",
      "150 0.2 0.580782681067959\n",
      "150 0.3 0.6784158229072461\n",
      "150 0.4 0.7064914544398682\n",
      "150 0.5 0.7285731323191933\n",
      "150 0.6 0.6979704173889704\n",
      "150 0.7 0.6908487618298794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v2f=videoname2text_features_sampled\n",
    "y_preds_all=[]\n",
    "total_preds=[[] for _ in range(len(deltas))]\n",
    "for videoname in videoname2AH_val:\n",
    "    frame2ind={}\n",
    "    for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "        frame2ind[frame]=ind\n",
    "    features=v2f[videoname]\n",
    "    X=[]\n",
    "    for frame,AH in content[videoname]['frame_annotation']:\n",
    "        ind=frame2ind[frame]\n",
    "        X.append(features[ind])\n",
    "        \n",
    "    preds_proba=mlpModelText.predict(np.array(X),verbose=0)\n",
    "    for hInd,delta in enumerate(deltas):\n",
    "        for i in range(len(preds_proba)):\n",
    "            i1=max(i-delta,0)\n",
    "            proba=np.mean(preds_proba[i1:i+delta+1])\n",
    "            total_preds[hInd].append(proba)\n",
    "\n",
    "for hInd,delta in enumerate(deltas):\n",
    "    for t in [0.2, 0.3,0.4,0.5,0.6, 0.7]:\n",
    "        new_pred = (np.array(total_preds[hInd]) >= t)\n",
    "        print(delta,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2 0.5494146363449106\n",
      "0 0.3 0.7203728655057625\n",
      "0 0.4 0.6968290348348019\n",
      "0 0.5 0.6768083342781502\n",
      "0 0.6 0.6742244189684848\n",
      "0 0.7 0.674065070583054\n",
      "\n",
      "5 0.2 0.5407562902521234\n",
      "5 0.3 0.7239266403698255\n",
      "5 0.4 0.6921089247413601\n",
      "5 0.5 0.6753978695492906\n",
      "5 0.6 0.674065070583054\n",
      "5 0.7 0.674065070583054\n",
      "\n",
      "10 0.2 0.5349881668811091\n",
      "10 0.3 0.7266921820194996\n",
      "10 0.4 0.6903031829266355\n",
      "10 0.5 0.6746754879641583\n",
      "10 0.6 0.674065070583054\n",
      "10 0.7 0.674065070583054\n",
      "\n",
      "20 0.2 0.5328460518731112\n",
      "20 0.3 0.7255505990645987\n",
      "20 0.4 0.6884670966684073\n",
      "20 0.5 0.6741978662556116\n",
      "20 0.6 0.674065070583054\n",
      "20 0.7 0.674065070583054\n",
      "\n",
      "30 0.2 0.5301124482127735\n",
      "30 0.3 0.7250098198287933\n",
      "30 0.4 0.6865440702811986\n",
      "30 0.5 0.674065070583054\n",
      "30 0.6 0.674065070583054\n",
      "30 0.7 0.674065070583054\n",
      "\n",
      "40 0.2 0.5282643827719866\n",
      "40 0.3 0.7269462609718909\n",
      "40 0.4 0.6873300620142411\n",
      "40 0.5 0.674065070583054\n",
      "40 0.6 0.674065070583054\n",
      "40 0.7 0.674065070583054\n",
      "\n",
      "50 0.2 0.5247548695173722\n",
      "50 0.3 0.726539084019141\n",
      "50 0.4 0.6869870387313755\n",
      "50 0.5 0.674065070583054\n",
      "50 0.6 0.674065070583054\n",
      "50 0.7 0.674065070583054\n",
      "\n",
      "60 0.2 0.5206159974032716\n",
      "60 0.3 0.7264699614775353\n",
      "60 0.4 0.6861284549307756\n",
      "60 0.5 0.674065070583054\n",
      "60 0.6 0.674065070583054\n",
      "60 0.7 0.674065070583054\n",
      "\n",
      "70 0.2 0.5160338546550342\n",
      "70 0.3 0.7254925270800054\n",
      "70 0.4 0.6837190168211641\n",
      "70 0.5 0.674065070583054\n",
      "70 0.6 0.674065070583054\n",
      "70 0.7 0.674065070583054\n",
      "\n",
      "80 0.2 0.5131119118160681\n",
      "80 0.3 0.7256560313044281\n",
      "80 0.4 0.6831881520129717\n",
      "80 0.5 0.674065070583054\n",
      "80 0.6 0.674065070583054\n",
      "80 0.7 0.674065070583054\n",
      "\n",
      "100 0.2 0.5109141158591237\n",
      "100 0.3 0.7240112714379672\n",
      "100 0.4 0.6820597025479354\n",
      "100 0.5 0.674065070583054\n",
      "100 0.6 0.674065070583054\n",
      "100 0.7 0.674065070583054\n",
      "\n",
      "150 0.2 0.506142732227883\n",
      "150 0.3 0.7212219827297398\n",
      "150 0.4 0.6816180432121721\n",
      "150 0.5 0.674065070583054\n",
      "150 0.6 0.674065070583054\n",
      "150 0.7 0.674065070583054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v2f=videoname2scores\n",
    "y_preds_all=[]\n",
    "total_preds=[[] for _ in range(len(deltas))]\n",
    "best_w1,best_w2=0.6,0.15\n",
    "for videoname in videoname2AH_val:\n",
    "    frame2ind={}\n",
    "    for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "        frame2ind[frame]=ind\n",
    "    video_features=videoname2scores[videoname]\n",
    "    audio_features=videoname2audio_features_sampled[videoname]\n",
    "    text_features=videoname2text_features_sampled[videoname]\n",
    "    X_video, X_audio, X_text=[],[],[]\n",
    "    for frame,AH in content[videoname]['frame_annotation']:\n",
    "        ind=frame2ind[frame]\n",
    "        X_video.append(video_features[ind])\n",
    "        X_audio.append(audio_features[ind])\n",
    "        X_text.append(text_features[ind])\n",
    "        \n",
    "    preds_proba_video=mlpModelFaces.predict(np.array(X_video),verbose=0)\n",
    "    preds_proba_audio=mlpModelAudio.predict(np.array(X_audio),verbose=0)\n",
    "    preds_proba_text=mlpModelText.predict(np.array(X_text),verbose=0)\n",
    "    y_ensemble=best_w1*preds_proba_video+best_w2*preds_proba_audio+(1-best_w1-best_w2)*preds_proba_text\n",
    "    for hInd,delta in enumerate(deltas):\n",
    "        for i in range(len(y_ensemble)):\n",
    "            i1=max(i-delta,0)\n",
    "            proba=np.mean(y_ensemble[i1:i+delta+1])\n",
    "            total_preds[hInd].append(proba)\n",
    "\n",
    "for hInd,delta in enumerate(deltas):\n",
    "    for t in [0.2,0.3,0.4,0.5,0.6,0.7]:\n",
    "        new_pred = (np.array(total_preds[hInd]) >= t)\n",
    "        print(delta,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89600,), 56769, (447, 1), (447, 1), (447, 1), (447, 1))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape,len(total_preds[hInd]),y_ensemble.shape,preds_proba_video.shape,preds_proba_audio.shape,preds_proba_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 4096) (127,)\n",
      "(304, 4096) (304,)\n"
     ]
    }
   ],
   "source": [
    "def get_video2target(v2f,videoname2AH):\n",
    "    X,y=[],[]\n",
    "    for videoname in videoname2AH:\n",
    "        frame2ind={}\n",
    "        for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "            frame2ind[frame]=ind\n",
    "        features=v2f[videoname]\n",
    "        cur_features=[]\n",
    "        for frame,AH in content[videoname]['frame_annotation']:\n",
    "            ind=frame2ind[frame]\n",
    "            cur_features.append(features[ind])\n",
    "        cur_features=np.array(cur_features)\n",
    "        mean_features = (np.mean(cur_features, axis=0))\n",
    "        std_features = (np.std(cur_features, axis=0))\n",
    "        max_features = (np.max(cur_features, axis=0))\n",
    "        min_features = (np.min(cur_features, axis=0))\n",
    "\n",
    "        # join several features together\n",
    "        feature = np.concatenate((mean_features, std_features, min_features, max_features), axis=None) \n",
    "        #feature=mean_features\n",
    "        X.append(feature)\n",
    "        y.append(videoname2AH[videoname])\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    print(X.shape,y.shape)\n",
    "    return X,y\n",
    "\n",
    "if False:\n",
    "    X_global_val,y_global_val=get_video2target(videoname2features,videoname2AH_val)\n",
    "    X_global_train,y_global_train=get_video2target(videoname2features,videoname2AH_train)\n",
    "elif False:\n",
    "    X_global_val,y_global_val=get_video2target(videoname2scores,videoname2AH_val)\n",
    "    X_global_train,y_global_train=get_video2target(videoname2scores,videoname2AH_train)\n",
    "else:\n",
    "    X_global_val,y_global_val=get_video2target(videoname2audio_features,videoname2AH_val)\n",
    "    X_global_train,y_global_train=get_video2target(videoname2audio_features,videoname2AH_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 768) (127,)\n",
      "(304, 768) (304,)\n"
     ]
    }
   ],
   "source": [
    "def get_text2target(videoname2AH,embeddings):\n",
    "    X,y=[],[]\n",
    "    for videoname in videoname2AH:\n",
    "        if False:\n",
    "            X.append(embeddings[videoname])\n",
    "        else:\n",
    "            X.append(embeddings[videoname].mean(axis=0))\n",
    "        y.append(videoname2AH[videoname])\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    print(X.shape,y.shape)\n",
    "    return X,y\n",
    "\n",
    "if True:\n",
    "    X_global_val,y_global_val=get_text2target(videoname2AH_val,text_embeddings)\n",
    "    X_global_train,y_global_train=get_text2target(videoname2AH_train,text_embeddings)\n",
    "else:\n",
    "    X_global_val,y_global_val=get_text2target(videoname2AH_val,videoname2audio_features)\n",
    "    X_global_train,y_global_train=get_text2target(videoname2AH_train,videoname2audio_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 768)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings['82557/Visite 1/82557_Question_1_2024-08-22 14-46-11_Video.mp4'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_global_train_norm=preprocessing.normalize(X_global_train,norm='l2')\n",
    "X_global_val_norm=preprocessing.normalize(X_global_val,norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7165354330708661\n"
     ]
    }
   ],
   "source": [
    "clf = svm.LinearSVC(C=1,class_weight='balanced') \n",
    "#clf = svm.SVC(C=1.2,kernel='rbf',class_weight='balanced')\n",
    "\n",
    "if True:    \n",
    "    clf.fit(X_global_train_norm, y_global_train)\n",
    "    y_global_pred = clf.predict(X_global_val_norm)\n",
    "else:\n",
    "    clf.fit(X_global_train, y_global_train)\n",
    "    y_global_pred = clf.predict(X_global_val)\n",
    "print(\"Accuracy:\",np.mean(y_global_val==y_global_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35, 19],\n",
       "       [23, 50]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_global_val,y_global_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6614173228346457\n",
      "0.2 0.6771653543307087\n",
      "0.3 0.6850393700787402\n",
      "0.4 0.6850393700787402\n",
      "0.5 0.6929133858267716\n",
      "0.6 0.6850393700787402\n",
      "0.7 0.6850393700787402\n",
      "0.7999999999999999 0.6850393700787402\n",
      "0.8999999999999999 0.6771653543307087\n",
      "0.9999999999999999 0.6771653543307087\n",
      "1.0999999999999999 0.6692913385826772\n",
      "1.2 0.6692913385826772\n",
      "1.3 0.6692913385826772\n",
      "1.4 0.6692913385826772\n",
      "1.5 0.6692913385826772\n",
      "1.5999999999999999 0.6850393700787402\n",
      "1.7 0.6850393700787402\n",
      "1.8 0.6850393700787402\n",
      "1.9 0.6850393700787402\n",
      "2.0 0.6850393700787402\n",
      "2.0999999999999996 0.6850393700787402\n",
      "2.1999999999999997 0.6850393700787402\n",
      "2.3 0.6850393700787402\n",
      "2.4 0.6850393700787402\n",
      "2.5 0.6850393700787402\n",
      "2.6 0.6850393700787402\n",
      "2.6999999999999997 0.6850393700787402\n",
      "2.8 0.6850393700787402\n",
      "2.9 0.6850393700787402\n",
      "3.0 0.6850393700787402\n",
      "3.0999999999999996 0.6850393700787402\n",
      "3.1999999999999997 0.6771653543307087\n",
      "3.3 0.6850393700787402\n",
      "3.4 0.6850393700787402\n",
      "3.5 0.6929133858267716\n",
      "3.5999999999999996 0.6929133858267716\n",
      "3.6999999999999997 0.6929133858267716\n",
      "3.8 0.6929133858267716\n",
      "3.9 0.6929133858267716\n",
      "4.0 0.6929133858267716\n"
     ]
    }
   ],
   "source": [
    "for C in np.linspace(0.1,4,40):\n",
    "    clf = svm.LinearSVC(C=C,class_weight='balanced') \n",
    "    if True:    \n",
    "        clf.fit(X_global_train_norm, y_global_train)\n",
    "        y_global_pred = clf.predict(X_global_val_norm)\n",
    "    else:\n",
    "        clf.fit(X_global_train, y_global_train)\n",
    "        y_global_pred = clf.predict(X_global_val)\n",
    "    print(C,np.mean(y_global_val==y_global_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6535433070866141\n",
      "0.2 0.6692913385826772\n",
      "0.3 0.7086614173228346\n",
      "0.4 0.7165354330708661\n",
      "0.5 0.7165354330708661\n",
      "0.6 0.7165354330708661\n",
      "0.7 0.7480314960629921\n",
      "0.7999999999999999 0.7480314960629921\n",
      "0.8999999999999999 0.7480314960629921\n",
      "0.9999999999999999 0.7559055118110236\n",
      "1.0999999999999999 0.7716535433070866\n",
      "1.2 0.7716535433070866\n",
      "1.3 0.7559055118110236\n",
      "1.4 0.7559055118110236\n",
      "1.5 0.7559055118110236\n",
      "1.5999999999999999 0.7480314960629921\n",
      "1.7 0.7401574803149606\n",
      "1.8 0.7322834645669292\n",
      "1.9 0.7322834645669292\n",
      "2.0 0.7165354330708661\n",
      "2.0999999999999996 0.7007874015748031\n",
      "2.1999999999999997 0.7007874015748031\n",
      "2.3 0.6929133858267716\n",
      "2.4 0.7007874015748031\n",
      "2.5 0.7086614173228346\n",
      "2.6 0.7007874015748031\n",
      "2.6999999999999997 0.7007874015748031\n",
      "2.8 0.7007874015748031\n",
      "2.9 0.7007874015748031\n",
      "3.0 0.7007874015748031\n",
      "3.0999999999999996 0.7007874015748031\n",
      "3.1999999999999997 0.7007874015748031\n",
      "3.3 0.6929133858267716\n",
      "3.4 0.6929133858267716\n",
      "3.5 0.6929133858267716\n",
      "3.5999999999999996 0.6929133858267716\n",
      "3.6999999999999997 0.6929133858267716\n",
      "3.8 0.6850393700787402\n",
      "3.9 0.6771653543307087\n",
      "4.0 0.6771653543307087\n"
     ]
    }
   ],
   "source": [
    "for C in np.linspace(0.1,4,40):\n",
    "    #clf = svm.LinearSVC(C=C,class_weight='balanced') \n",
    "    clf = svm.SVC(C=C,kernel='rbf',class_weight='balanced')\n",
    "    if False:    \n",
    "        clf.fit(X_global_train_norm, y_global_train)\n",
    "        y_global_pred = clf.predict(X_global_val_norm)\n",
    "    else:\n",
    "        clf.fit(X_global_train, y_global_train)\n",
    "        y_global_pred = clf.predict(X_global_val)\n",
    "    print(C,np.mean(y_global_val==y_global_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7716535433070866 F1: 0.7713631924180372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[39, 15],\n",
       "       [14, 59]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = svm.LinearSVC(C=1,class_weight='balanced') \n",
    "clf = svm.SVC(C=1.1,kernel='rbf',class_weight='balanced')\n",
    "if False:    \n",
    "    clf.fit(X_global_train_norm, y_global_train)\n",
    "    y_global_pred = clf.predict(X_global_val_norm)\n",
    "else:\n",
    "    clf.fit(X_global_train, y_global_train)\n",
    "    y_global_pred = clf.predict(X_global_val)\n",
    "print(\"Accuracy:\",np.mean(y_global_val==y_global_pred), \"F1:\",f1_score(y_true=y_global_val,y_pred=y_global_pred, average=\"weighted\"))\n",
    "confusion_matrix(y_global_val,y_global_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(431, 768) (431,)\n",
      "Accuracy: 0.7952755905511811 F1: 0.7960479411904499\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_VAL:\n",
    "    TRAIN_VAL=True\n",
    "    X_global_all=np.concatenate((X_global_train,X_global_val))\n",
    "    y_global_all=np.concatenate((y_global_train,y_global_val))\n",
    "    print(X_global_all.shape,y_global_all.shape)\n",
    "\n",
    "    clf = svm.SVC(C=1.1,kernel='rbf',class_weight='balanced')\n",
    "    clf.fit(X_global_all, y_global_all)\n",
    "    y_global_pred = clf.predict(X_global_val)\n",
    "    print(\"Accuracy:\",np.mean(y_global_val==y_global_pred), \"F1:\",f1_score(y_true=y_global_val,y_pred=y_global_pred, average=\"weighted\"))\n",
    "    confusion_matrix(y_global_val,y_global_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2 0.669449297761579\n",
      "0 0.3 0.7206922005883483\n",
      "0 0.4 0.6938581403535673\n",
      "0 0.5 0.6807216079995162\n",
      "0 0.6 0.6748753685941896\n",
      "\n",
      "5 0.2 0.6611912640510336\n",
      "5 0.3 0.7209018972302281\n",
      "5 0.4 0.6897304975992095\n",
      "5 0.5 0.6782404450654532\n",
      "5 0.6 0.674065070583054\n",
      "\n",
      "10 0.2 0.6571318538522992\n",
      "10 0.3 0.7211785274063173\n",
      "10 0.4 0.687517561750827\n",
      "10 0.5 0.6763282877330855\n",
      "10 0.6 0.674065070583054\n",
      "\n",
      "20 0.2 0.6519277619505952\n",
      "20 0.3 0.7197824642470669\n",
      "20 0.4 0.684170430534739\n",
      "20 0.5 0.6741539086200914\n",
      "20 0.6 0.674065070583054\n",
      "\n",
      "30 0.2 0.6500169427713312\n",
      "30 0.3 0.7158727619497623\n",
      "30 0.4 0.6809856058694501\n",
      "30 0.5 0.674065070583054\n",
      "30 0.6 0.674065070583054\n",
      "\n",
      "40 0.2 0.6485636404001724\n",
      "40 0.3 0.7136276678594953\n",
      "40 0.4 0.6783449806315937\n",
      "40 0.5 0.674065070583054\n",
      "40 0.6 0.674065070583054\n",
      "\n",
      "50 0.2 0.6456210468146547\n",
      "50 0.3 0.7105147344001636\n",
      "50 0.4 0.6784188941724052\n",
      "50 0.5 0.674065070583054\n",
      "50 0.6 0.674065070583054\n",
      "\n",
      "60 0.2 0.6439890442689946\n",
      "60 0.3 0.7091157417311046\n",
      "60 0.4 0.6780097854742909\n",
      "60 0.5 0.674065070583054\n",
      "60 0.6 0.674065070583054\n",
      "\n",
      "70 0.2 0.6418823321879426\n",
      "70 0.3 0.7067902240922671\n",
      "70 0.4 0.6770126639749056\n",
      "70 0.5 0.674065070583054\n",
      "70 0.6 0.674065070583054\n",
      "\n",
      "80 0.2 0.6390905387086665\n",
      "80 0.3 0.7035065618134992\n",
      "80 0.4 0.6768543238216044\n",
      "80 0.5 0.674065070583054\n",
      "80 0.6 0.674065070583054\n",
      "\n",
      "100 0.2 0.6369209111640717\n",
      "100 0.3 0.7011469657859993\n",
      "100 0.4 0.6757343965144307\n",
      "100 0.5 0.674065070583054\n",
      "100 0.6 0.674065070583054\n",
      "\n",
      "150 0.2 0.6303813520040126\n",
      "150 0.3 0.6949561662714684\n",
      "150 0.4 0.6744102280626715\n",
      "150 0.5 0.674065070583054\n",
      "150 0.6 0.674065070583054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#v2f=videoname2features\n",
    "v2f=videoname2scores\n",
    "total_preds=[[] for _ in range(len(deltas))]\n",
    "\n",
    "for videoname in videoname2AH_val:\n",
    "    frame2ind={}\n",
    "    for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "        frame2ind[frame]=ind\n",
    "    features=v2f[videoname]\n",
    "    #text_embs=text_embeddings[videoname]\n",
    "    text_embs=text_embeddings[videoname].mean(axis=0)\n",
    "    global_AH=clf.predict([text_embs])[0]\n",
    "    X=[]\n",
    "    for frame,AH in content[videoname]['frame_annotation']:\n",
    "        ind=frame2ind[frame]\n",
    "        X.append(features[ind])\n",
    "        #X.append(np.concatenate((features[ind],text_embs),axis=0))\n",
    "        \n",
    "        \n",
    "    preds_proba=mlpModelFaces.predict(np.array(X),verbose=0)\n",
    "    for hInd,delta in enumerate(deltas):\n",
    "        for i in range(len(preds_proba)):\n",
    "            i1=max(i-delta,0)\n",
    "            proba=np.mean(preds_proba[i1:i+delta+1])\n",
    "            if global_AH==0:\n",
    "                proba=0\n",
    "            total_preds[hInd].append(proba)\n",
    "            \n",
    "\n",
    "for hInd,delta in enumerate(deltas):\n",
    "    for t in [0.2, 0.3,0.4,0.5,0.6]:\n",
    "        new_pred = (np.array(total_preds[hInd]) >= t)\n",
    "        print(delta,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2 0.6396827050351916\n",
      "0 0.3 0.7260132081425393\n",
      "0 0.4 0.6971369077680634\n",
      "0 0.5 0.6768083342781502\n",
      "0 0.6 0.6742244189684848\n",
      "0 0.7 0.674065070583054\n",
      "\n",
      "5 0.2 0.6313707218841565\n",
      "5 0.3 0.727732307337362\n",
      "5 0.4 0.6924127464977677\n",
      "5 0.5 0.6753978695492906\n",
      "5 0.6 0.674065070583054\n",
      "5 0.7 0.674065070583054\n",
      "\n",
      "10 0.2 0.6268135889210799\n",
      "10 0.3 0.7300575035301405\n",
      "10 0.4 0.6905900347362406\n",
      "10 0.5 0.6746754879641583\n",
      "10 0.6 0.674065070583054\n",
      "10 0.7 0.674065070583054\n",
      "\n",
      "20 0.2 0.6245487373913897\n",
      "20 0.3 0.7283739429143603\n",
      "20 0.4 0.6887125897202173\n",
      "20 0.5 0.6741978662556116\n",
      "20 0.6 0.674065070583054\n",
      "20 0.7 0.674065070583054\n",
      "\n",
      "30 0.2 0.6206014337151411\n",
      "30 0.3 0.7273446683273185\n",
      "30 0.4 0.6867056310880202\n",
      "30 0.5 0.674065070583054\n",
      "30 0.6 0.674065070583054\n",
      "30 0.7 0.674065070583054\n",
      "\n",
      "40 0.2 0.6197491440102847\n",
      "40 0.3 0.7290357253536482\n",
      "40 0.4 0.6873922546268629\n",
      "40 0.5 0.674065070583054\n",
      "40 0.6 0.674065070583054\n",
      "40 0.7 0.674065070583054\n",
      "\n",
      "50 0.2 0.6166659808810205\n",
      "50 0.3 0.7287388104099725\n",
      "50 0.4 0.6869870387313755\n",
      "50 0.5 0.674065070583054\n",
      "50 0.6 0.674065070583054\n",
      "50 0.7 0.674065070583054\n",
      "\n",
      "60 0.2 0.613780539400344\n",
      "60 0.3 0.728897266482514\n",
      "60 0.4 0.6861284549307756\n",
      "60 0.5 0.674065070583054\n",
      "60 0.6 0.674065070583054\n",
      "60 0.7 0.674065070583054\n",
      "\n",
      "70 0.2 0.6101648157667316\n",
      "70 0.3 0.7277884329373431\n",
      "70 0.4 0.6837190168211641\n",
      "70 0.5 0.674065070583054\n",
      "70 0.6 0.674065070583054\n",
      "70 0.7 0.674065070583054\n",
      "\n",
      "80 0.2 0.607758533381641\n",
      "80 0.3 0.7278605500746482\n",
      "80 0.4 0.6831881520129717\n",
      "80 0.5 0.674065070583054\n",
      "80 0.6 0.674065070583054\n",
      "80 0.7 0.674065070583054\n",
      "\n",
      "100 0.2 0.6056634750804321\n",
      "100 0.3 0.726260114057261\n",
      "100 0.4 0.6820597025479354\n",
      "100 0.5 0.674065070583054\n",
      "100 0.6 0.674065070583054\n",
      "100 0.7 0.674065070583054\n",
      "\n",
      "150 0.2 0.6014350556113524\n",
      "150 0.3 0.7237252323701164\n",
      "150 0.4 0.6816180432121721\n",
      "150 0.5 0.674065070583054\n",
      "150 0.6 0.674065070583054\n",
      "150 0.7 0.674065070583054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v2f=videoname2scores\n",
    "y_preds_all=[]\n",
    "total_preds=[[] for _ in range(len(deltas))]\n",
    "best_w1,best_w2=0.6,0.15\n",
    "for videoname in videoname2AH_val:\n",
    "    frame2ind={}\n",
    "    for ind,frame in enumerate(videoname2img_names[videoname]):\n",
    "        frame2ind[frame]=ind\n",
    "    \n",
    "    text_embs=text_embeddings[videoname].mean(axis=0)\n",
    "    global_AH=clf.predict([text_embs])[0]\n",
    "    \n",
    "    video_features=videoname2scores[videoname]\n",
    "    audio_features=videoname2audio_features_sampled[videoname]\n",
    "    text_features=videoname2text_features_sampled[videoname]\n",
    "    X_video, X_audio, X_text=[],[],[]\n",
    "    for frame,AH in content[videoname]['frame_annotation']:\n",
    "        ind=frame2ind[frame]\n",
    "        X_video.append(video_features[ind])\n",
    "        X_audio.append(audio_features[ind])\n",
    "        X_text.append(text_features[ind])\n",
    "        \n",
    "    preds_proba_video=mlpModelFaces.predict(np.array(X_video),verbose=0)\n",
    "    preds_proba_audio=mlpModelAudio.predict(np.array(X_audio),verbose=0)\n",
    "    preds_proba_text=mlpModelText.predict(np.array(X_text),verbose=0)\n",
    "    y_ensemble=best_w1*preds_proba_video+best_w2*preds_proba_audio+(1-best_w1-best_w2)*preds_proba_text\n",
    "    for hInd,delta in enumerate(deltas):\n",
    "        for i in range(len(y_ensemble)):\n",
    "            i1=max(i-delta,0)\n",
    "            proba=np.mean(y_ensemble[i1:i+delta+1])\n",
    "            if global_AH==0:\n",
    "                proba=0\n",
    "            total_preds[hInd].append(proba)\n",
    "\n",
    "for hInd,delta in enumerate(deltas):\n",
    "    for t in [0.2,0.3,0.4,0.5,0.6,0.7]:\n",
    "        new_pred = (np.array(total_preds[hInd]) >= t)\n",
    "        print(delta,t,f1_score(y_val,new_pred,average='weighted'))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save test results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169479 ['82683/Visite_1/82683_Question_1_2024-11-14_09-21-32_Video.mp4/frame-0.jpg,0', '82683/Visite_1/82683_Question_1_2024-11-14_09-21-32_Video.mp4/frame-1.jpg,0', '82683/Visite_1/82683_Question_1_2024-11-14_09-21-32_Video.mp4/frame-2.jpg,0', '82683/Visite_1/82683_Question_1_2024-11-14_09-21-32_Video.mp4/frame-3.jpg,0', '82683/Visite_1/82683_Question_1_2024-11-14_09-21-32_Video.mp4/frame-4.jpg,0']\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(TEST_DATA_DIR,'submissions/trial-1.txt'),'r') as f:\n",
    "    test_set_sample=f.read().splitlines()\n",
    "print(len(test_set_sample),test_set_sample[:5])\n",
    "\n",
    "test_set_videos=[]\n",
    "for s in test_set_sample:\n",
    "    videoname=os.path.dirname(s[:-2])\n",
    "    if videoname not in test_set_videos:\n",
    "        test_set_videos.append(videoname)\n",
    "    \n",
    "print(len(test_set_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/HDD6TB/datasets/emotions/ABAW/ABAW_8/ABAW-8th-BAH-train-data/ABAW-8th-BAH-test-data/outputs\n"
     ]
    }
   ],
   "source": [
    "OUTDIR=os.path.join(TEST_DATA_DIR,'outputs')\n",
    "print(OUTDIR)\n",
    "if not os.path.exists(OUTDIR):\n",
    "    os.makedirs(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169479,) 15329\n"
     ]
    }
   ],
   "source": [
    "v2f=videoname2scores_test\n",
    "delta=20\n",
    "t=0.3\n",
    "y_preds_all=[]\n",
    "total_preds=[]\n",
    "for videoname in test_set_videos:\n",
    "    features=v2f[videoname]\n",
    "        \n",
    "    preds_proba=mlpModelFaces.predict(features,verbose=0)\n",
    "    for i in range(len(preds_proba)):\n",
    "        i1=max(i-delta,0)\n",
    "        proba=np.mean(preds_proba[i1:i+delta+1])\n",
    "        total_preds.append(proba)\n",
    "\n",
    "new_pred = 1*(np.array(total_preds) >= t)\n",
    "print(new_pred.shape,new_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=0\n",
    "#with open(os.path.join(OUTDIR,'mbf_va.txt'), 'w') as f:\n",
    "with open(os.path.join(OUTDIR,'mbf_va_train_val.txt'), 'w') as f:\n",
    "    for videoname in test_set_videos:\n",
    "        for img_name in videoname2img_names_test[videoname]:\n",
    "            f.write(img_name+','+str(new_pred[ind])+'\\n') #\n",
    "            ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169479,) 18367\n"
     ]
    }
   ],
   "source": [
    "delta=10\n",
    "t=0.3\n",
    "y_preds_all=[]\n",
    "total_preds=[]\n",
    "best_w1,best_w2=0.6,0.15\n",
    "for videoname in test_set_videos:\n",
    "    video_features=videoname2scores_test[videoname]\n",
    "    audio_features=videoname2audio_features_sampled_test[videoname]\n",
    "    text_features=videoname2text_features_sampled_test[videoname]\n",
    "    \n",
    "    preds_proba_video=mlpModelFaces.predict(np.array(video_features),verbose=0)\n",
    "    preds_proba_audio=mlpModelAudio.predict(np.array(audio_features),verbose=0)\n",
    "    preds_proba_text=mlpModelText.predict(np.array(text_features),verbose=0)\n",
    "    preds_ensemble=best_w1*preds_proba_video+best_w2*preds_proba_audio+(1-best_w1-best_w2)*preds_proba_text\n",
    "    for i in range(len(preds_ensemble)):\n",
    "        i1=max(i-delta,0)\n",
    "        proba=np.mean(preds_ensemble[i1:i+delta+1])\n",
    "        total_preds.append(proba)\n",
    "\n",
    "new_pred = 1*(np.array(total_preds) >= t)\n",
    "print(new_pred.shape,new_pred.sum()) #21325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=0\n",
    "#with open(os.path.join(OUTDIR,'mbf_va_hubert_roberta.txt'), 'w') as f:\n",
    "with open(os.path.join(OUTDIR,'mbf_va_hubert_roberta_train_val.txt'), 'w') as f:\n",
    "    for videoname in test_set_videos:\n",
    "        for img_name in videoname2img_names_test[videoname]:\n",
    "            f.write(img_name+','+str(new_pred[ind])+'\\n') #\n",
    "            ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169479,) 16051\n"
     ]
    }
   ],
   "source": [
    "delta=10\n",
    "t=0.3\n",
    "y_preds_all=[]\n",
    "total_preds=[]\n",
    "best_w1,best_w2=0.6,0.15\n",
    "for videoname in test_set_videos:\n",
    "    video_features=videoname2scores_test[videoname]\n",
    "    audio_features=videoname2audio_features_sampled_test[videoname]\n",
    "    text_features=videoname2text_features_sampled_test[videoname]\n",
    "\n",
    "    text_embs=text_embeddings_test[videoname].mean(axis=0)\n",
    "    global_AH=clf.predict([text_embs])[0]\n",
    "    \n",
    "    preds_proba_video=mlpModelFaces.predict(np.array(video_features),verbose=0)\n",
    "    preds_proba_audio=mlpModelAudio.predict(np.array(audio_features),verbose=0)\n",
    "    preds_proba_text=mlpModelText.predict(np.array(text_features),verbose=0)\n",
    "    preds_ensemble=best_w1*preds_proba_video+best_w2*preds_proba_audio+(1-best_w1-best_w2)*preds_proba_text\n",
    "    for i in range(len(preds_ensemble)):\n",
    "        i1=max(i-delta,0)\n",
    "        proba=np.mean(preds_ensemble[i1:i+delta+1])\n",
    "        if global_AH==0:\n",
    "            proba=0\n",
    "        total_preds.append(proba)\n",
    "\n",
    "new_pred = 1*(np.array(total_preds) >= t)\n",
    "print(new_pred.shape,new_pred.sum()) #20044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=0\n",
    "#with open(os.path.join(OUTDIR,'mbf_va_hubert_roberta_filter.txt'), 'w') as f:\n",
    "with open(os.path.join(OUTDIR,'mbf_va_hubert_roberta_filter_train_val.txt'), 'w') as f:\n",
    "    for videoname in test_set_videos:\n",
    "        for img_name in videoname2img_names_test[videoname]:\n",
    "            f.write(img_name+','+str(new_pred[ind])+'\\n') #\n",
    "            ind+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
